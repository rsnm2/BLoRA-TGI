{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minimal-llama/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig, set_peft_model_state_dict\n",
    "import peft\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import inspect\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"jondurbin/airoboros-7b-gpt4-1.2-peft\", adapter_name=\"airoboros\")\n",
    "model = PeftModel.from_pretrained(model.base_model.model, \"trl-lib/llama-7b-se-rl-peft\", adapter_name=\"se-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    module.batch_lora_ids = [\"airoboros\", \"se-rl\"]\n",
    "\n",
    "# set(type(module) for _, module in model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer(\"The LLaMA language model is\", return_tensors=\"pt\")\n",
    "b = torch.cat([batch['input_ids'], batch['input_ids']], dim=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  The LLaMA language model is a large-scale neural network trained on a large corpus of text. It can be used for various tasks such as text classification, text generation, and natural language understanding.\n",
      "\n",
      "## Architecture\n",
      "\n",
      "The LLaMA model consists of an encoder-decoder architecture with attention mechanism. The encoder part is a bidirectional Gated Recurrent Unit (GRU) network, while the decoder part is a simple feed-forward network. The attention mechanism is used to focus on important parts of the input sequence during decoding.\n",
      "\n",
      "## Training\n",
      "\n",
      "The LLaMA model is trained using the negative sampling technique. In this approach, the model is trained on a large corpus of text and then used to generate new sentences. To generate new sentences, the model first generates a set of candidate sentences using the encoder part. Then, it selects the best candidate sentence based on its probability score. The probability score is calculated by\n"
     ]
    }
   ],
   "source": [
    "from blora import forward\n",
    "Linear.forward = forward\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model.generate(\n",
    "        input_ids=b,\n",
    "        attention_mask=torch.ones_like(b),\n",
    "        max_length=200,\n",
    "        # batch_lora_ids=[\"airoboros\", \"se-rl\"],\n",
    "    )\n",
    "print(tokenizer.decode(out1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  The LLaMA language model is a large-scale neural network trained on a large corpus of text. It can be used for various tasks such as text classification, text generation, and natural language understanding.\n",
      "\n",
      "## Architecture\n",
      "\n",
      "The LLaMA model consists of an encoder-decoder architecture with attention mechanism. The encoder part is a bidirectional Gated Recurrent Unit (GRU) network, while the decoder part is a simple feed-forward network. The attention mechanism is used to focus on important parts of the input sequence during decoding.\n",
      "\n",
      "## Training\n",
      "\n",
      "The LLaMA model is trained using the negative sampling technique. In this approach, the model is trained on a large corpus of text and then used to generate new sentences. To generate new sentences, the model first generates a set of candidate sentences using the encoder part. Then, it selects the best candidate sentence based on its probability score. The probability score is calculated by\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLa\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, l, c = 2, 10, 1024\n",
    "r = 8\n",
    "\n",
    "X1 = torch.randn(l, c)\n",
    "X2 = torch.randn(l, c)\n",
    "X = torch.cat([X1, X2], dim=0)\n",
    "XX = torch.cat([X1, X2], dim=1)\n",
    "\n",
    "weight = torch.randn(([c, c]))\n",
    "\n",
    "A1 = torch.randn(c, r)\n",
    "A2 = torch.randn(c, r)\n",
    "A = torch.cat([A1, A2], dim=1)\n",
    "AA = torch.cat([torch.cat([A1, torch.zeros_like(A1)]), torch.cat([torch.zeros_like(A2), A2])], dim=1)\n",
    "Y = X @ A\n",
    "\n",
    "B1 = torch.randn(c, r)\n",
    "B2 = torch.randn(c, r)\n",
    "BB = torch.cat([torch.cat([B1, torch.zeros_like(B1)]), torch.cat([torch.zeros_like(B2), B2])], dim=1)\n",
    "\n",
    "# ((XX @ AA) @ BB.T)[:,:c] - ((X1 @ A1) @ B1.T)\n",
    "# ((XX @ AA) @ BB.T)[:,c:] - ((X2 @ A2) @ B2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora0: list,\n",
    "        lora1: list,\n",
    "        weight: torch.Tensor,\n",
    "        scaling: float = 4.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lora_a = torch.cat([torch.cat([lora0[0], torch.zeros_like(lora0[0])]), torch.cat([torch.zeros_like(lora1[0]), lora1[0]])], dim=1).to('cuda:0')\n",
    "        self.lora_b = torch.cat([torch.cat([lora0[1], torch.zeros_like(lora0[1])]), torch.cat([torch.zeros_like(lora1[1]), lora1[1]])], dim=1).to('cuda:0')\n",
    "        self.weight = weight.to('cuda:0')\n",
    "        self.scaling = scaling\n",
    "        self.ctx_size = self.weight.shape[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = F.linear(x, transpose(self.weight, False), bias=None)\n",
    "        x = torch.cat([x[0], x[1]], dim=1)\n",
    "\n",
    "        out = self.scaling * (x @ self.lora_a) @ self.lora_b.T\n",
    "        out = torch.stack([out[:, :self.ctx_size], out[:, self.ctx_size:]])\n",
    "        result += out\n",
    "        return result\n",
    "\n",
    "\n",
    "b, l, c = 2, 10, 1024\n",
    "r = 8\n",
    "\n",
    "X1 = torch.randn(l, c)\n",
    "X2 = torch.randn(l, c)\n",
    "X = torch.stack([X1, X2])\n",
    "weight = torch.randn(([c, c]))\n",
    "\n",
    "A1 = torch.randn(c, r)\n",
    "A2 = torch.randn(c, r)\n",
    "\n",
    "B1 = torch.randn(c, r)\n",
    "B2 = torch.randn(c, r)\n",
    "\n",
    "AA = torch.cat([torch.cat([A1, torch.zeros_like(A1)]), torch.cat([torch.zeros_like(A2), A2])], dim=1)\n",
    "blora = BLora(lora0=[A1, B1], lora1=[A2, B2], weight=weight)\n",
    "\n",
    "X.to('cuda:0')\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = blora(X)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn, std_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        blora(X)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_forward(x, weight, lora_A, lora_B, scaling):\n",
    "    result = F.linear(x, transpose(weight, False))\n",
    "    result += (\n",
    "            (x @ lora_A) @ lora_B.T\n",
    "        * scaling\n",
    "    )\n",
    "\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        lora_forward(X1, weight, A1, B1, 4.0); \n",
    "        lora_forward(X2, weight, A2, B2, 4.0)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn, std_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 10\n",
    "ctx_dim = 4096\n",
    "rank = 8\n",
    "scaling = 4.0\n",
    "fan_in_fan_out = False\n",
    "bias = None\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim], device=device)\n",
    "x1 = torch.randn([bs, seq_len, ctx_dim], device=device)\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]), device=device)\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False, device=device)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False, device=device)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False, device=device)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False, device=device)\n",
    "\n",
    "loraa = torch.cat([lora_a0.weight, lora_a1.weight], dim=1)\n",
    "lorab = torch.cat([lora_b0.weight, lora_b1.weight], dim=0)\n",
    "\n",
    "# forward pass\n",
    "def lora_forward(x, weight, lora1, lora2, scaling):\n",
    "    result = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "\n",
    "    if isinstance(lora1, torch.nn.Linear):\n",
    "        x = x.to(lora1.weight.dtype)\n",
    "        result += scaling * lora2(lora1(x))\n",
    "        return result\n",
    "    else:\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(lora1.dtype)\n",
    "        out = F.linear(x, transpose(lora1, fan_in_fan_out), bias=bias)\n",
    "        out = scaling * F.linear(out, transpose(lora2, fan_in_fan_out), bias=bias)\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        result += out\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "r0 = lora_forward(x0, weight, lora_a0, lora_b0, scaling)\n",
    "r1 = lora_forward(x1, weight, lora_a1, lora_b1, scaling)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora1: list,\n",
    "        lora2: list,\n",
    "        weight: torch.Tensor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lora_a = torch.nn.Parameter(torch.cat([lora1[0].weight, lora2[0].weight], dim=1))\n",
    "        self.lora_b = torch.nn.Parameter(torch.cat([lora1[1].weight, lora2[1].weight], dim=0))\n",
    "        self.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = F.linear(x, transpose(self.weight, fan_in_fan_out), bias=bias)\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(self.lora_a.dtype)\n",
    "\n",
    "        out = F.linear(x, transpose(self.lora_a, fan_in_fan_out), bias=bias)\n",
    "        out = scaling * F.linear(out, transpose(self.lora_b, fan_in_fan_out), bias=bias)\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        result += out\n",
    "        return result\n",
    "    \n",
    "blora = BLora(lora1=[lora_a0, lora_b0], lora2=[lora_a1, lora_b1], weight=weight)\n",
    "\n",
    "start = time.time()\n",
    "r2 = blora(x0)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora1 = torch.nn.Linear(in_features=bs * ctx_dim, out_features=rank, bias=False)\n",
    "lora2 = torch.nn.Linear(in_features=rank, out_features=bs * ctx_dim, bias=False)\n",
    "\n",
    "lora1.weight = torch.nn.Parameter(torch.cat([lora_a0.weight, lora_a1.weight], dim=1))\n",
    "lora2.weight = torch.nn.Parameter(torch.cat([lora_b0.weight, lora_b1.weight], dim=0))\n",
    "\n",
    "# forward pass\n",
    "result1 = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "x1 = x0.reshape(seq_len, -1)\n",
    "x1 = x1.to(lora1.weight.dtype)\n",
    "out1 = lora2(lora1(x1)) * scaling\n",
    "out1 = out1.reshape(bs, seq_len, -1)\n",
    "result1 += out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_model.bin\"\n",
    "peft_model_state_dict = torch.load(path)\n",
    "\n",
    "peft_config_path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_config.json\"\n",
    "peft_config = PeftConfig.from_json_file(peft_config_path)\n",
    "peft_config = {'default' : peft_config}\n",
    "model.peft_config = peft_config\n",
    "set_peft_model_state_dict(model, peft_model_state_dict, adapter_name=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
