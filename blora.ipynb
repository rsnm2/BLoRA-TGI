{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig, set_peft_model_state_dict\n",
    "import peft\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:10<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.view vs torch.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, l, c = 2, 10, 1024\n",
    "r = 8\n",
    "\n",
    "X1 = torch.randn(l, c)\n",
    "X2 = torch.randn(l, c)\n",
    "X = torch.cat([X1, X2], dim=0)\n",
    "XX = torch.cat([X1, X2], dim=1)\n",
    "\n",
    "weight = torch.randn(([c, c]))\n",
    "\n",
    "A1 = torch.randn(c, r)\n",
    "A2 = torch.randn(c, r)\n",
    "A = torch.cat([A1, A2], dim=1)\n",
    "AA = torch.cat([torch.cat([A1, torch.zeros_like(A1)]), torch.cat([torch.zeros_like(A2), A2])], dim=1)\n",
    "Y = X @ A\n",
    "\n",
    "B1 = torch.randn(c, r)\n",
    "B2 = torch.randn(c, r)\n",
    "BB = torch.cat([torch.cat([B1, torch.zeros_like(B1)]), torch.cat([torch.zeros_like(B2), B2])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 16])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -40.8750,  -73.5000,  -59.5625,  ...,  213.5000,  -36.7500,\n",
       "           66.3750],\n",
       "        [ -71.8750,  -89.8750,  196.7500,  ...,   39.6562, -117.1250,\n",
       "           26.3750],\n",
       "        [  17.0625, -116.9375, -179.1250,  ...,  -44.9375,   22.2656,\n",
       "          -79.7500],\n",
       "        ...,\n",
       "        [-127.3125,   37.0312,  -29.5312,  ...,  -99.2500,   44.3125,\n",
       "         -145.6250],\n",
       "        [  51.8125,   79.1875,   66.3125,  ..., -100.4375,  130.5000,\n",
       "          -48.3750],\n",
       "        [ -43.2812,  -59.6562,  175.6250,  ...,  -70.1875,  102.3125,\n",
       "         -119.3125]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(XX @ AA) @ BB.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((XX @ AA) @ BB.T)[:,:c] - ((X1 @ A1) @ B1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((XX @ AA) @ BB.T)[:,c:] - ((X2 @ A2) @ B2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22445194671551386 0.011198871724307558\n"
     ]
    }
   ],
   "source": [
    "class BLora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora0: list,\n",
    "        lora1: list,\n",
    "        weight: torch.Tensor,\n",
    "        scaling: float = 4.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lora_a = torch.cat([torch.cat([lora0[0], torch.zeros_like(lora0[0])]), torch.cat([torch.zeros_like(lora1[0]), lora1[0]])], dim=1).to('cuda:0')\n",
    "        self.lora_b = torch.cat([torch.cat([lora0[1], torch.zeros_like(lora0[1])]), torch.cat([torch.zeros_like(lora1[1]), lora1[1]])], dim=1).to('cuda:0')\n",
    "        self.weight = weight.to('cuda:0')\n",
    "        self.scaling = scaling\n",
    "        self.ctx_size = self.weight.shape[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = F.linear(x, transpose(self.weight, False), bias=None)\n",
    "        x = torch.cat([x[0], x[1]], dim=1)\n",
    "\n",
    "        out = self.scaling * (x @ self.lora_a) @ self.lora_b.T\n",
    "        out = torch.stack([out[:, :self.ctx_size], out[:, self.ctx_size:]])\n",
    "        result += out\n",
    "        return result\n",
    "\n",
    "\n",
    "b, l, c = 2, 10, 1024\n",
    "r = 8\n",
    "\n",
    "X1 = torch.randn(l, c)\n",
    "X2 = torch.randn(l, c)\n",
    "X = torch.stack([X1, X2])\n",
    "weight = torch.randn(([c, c]))\n",
    "\n",
    "A1 = torch.randn(c, r)\n",
    "A2 = torch.randn(c, r)\n",
    "\n",
    "B1 = torch.randn(c, r)\n",
    "B2 = torch.randn(c, r)\n",
    "\n",
    "AA = torch.cat([torch.cat([A1, torch.zeros_like(A1)]), torch.cat([torch.zeros_like(A2), A2])], dim=1)\n",
    "blora = BLora(lora0=[A1, B1], lora1=[A2, B2], weight=weight)\n",
    "\n",
    "X.to('cuda:0')\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = blora(X)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn, std_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        14.75%     356.000us        99.34%       2.398ms       2.398ms       0.000us         0.00%      52.000us      52.000us             1  \n",
      "                                           aten::matmul         0.87%      21.000us        74.65%       1.802ms     600.667us       0.000us         0.00%      31.000us      10.333us             3  \n",
      "                                               aten::mm        69.64%       1.681ms        72.87%       1.759ms     586.333us      31.000us        59.62%      31.000us      10.333us             3  \n",
      "                                           aten::linear         0.29%       7.000us        72.74%       1.756ms       1.756ms       0.000us         0.00%      12.000us      12.000us             1  \n",
      "                                       cudaLaunchKernel         4.64%     112.000us         4.64%     112.000us      14.000us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "                                              aten::cat         1.70%      41.000us         2.82%      68.000us      34.000us      13.000us        25.00%      13.000us       6.500us             2  \n",
      "                                                aten::t         1.04%      25.000us         1.70%      41.000us      41.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                              aten::mul         1.24%      30.000us         1.62%      39.000us      39.000us       4.000us         7.69%       4.000us       4.000us             1  \n",
      "                                            aten::stack         0.46%      11.000us         1.45%      35.000us      35.000us       0.000us         0.00%       7.000us       7.000us             1  \n",
      "                                             aten::add_         1.04%      25.000us         1.37%      33.000us      33.000us       4.000us         7.69%       4.000us       4.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.414ms\n",
      "Self CUDA time total: 52.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        blora(X)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21042741348346075 0.016543566806106854\n"
     ]
    }
   ],
   "source": [
    "def lora_forward(x, weight, lora_A, lora_B, scaling):\n",
    "    result = F.linear(x, transpose(weight, False))\n",
    "    result += (\n",
    "            (x @ lora_A) @ lora_B.T\n",
    "        * scaling\n",
    "    )\n",
    "\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        lora_forward(X1, weight, A1, B1, 4.0); \n",
    "        lora_forward(X2, weight, A2, B2, 4.0)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn, std_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X1.cuda()\n",
    "A1 = A1.cuda()\n",
    "B1 = B1.cuda()\n",
    "weight = weight.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.7 µs ± 604 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X1 @ weight.T + 4.0 * (X1 @ A1) @ B1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 430.7500, -140.5000,  344.5000,  ...,   40.5938,  -86.9375,\n",
       "         -355.5000],\n",
       "        [-242.2500, -188.2500,  123.8125,  ...,  301.2500,  357.2500,\n",
       "           75.6875],\n",
       "        [-377.2500,  427.0000, -346.0000,  ..., -112.5000,  110.1875,\n",
       "          486.7500],\n",
       "        ...,\n",
       "        [ 269.5000, -397.2500, -450.7500,  ..., -267.0000, -178.3750,\n",
       "         -190.2500],\n",
       "        [ 281.2500, -616.5000,   56.9375,  ..., -119.5625,  108.5000,\n",
       "         -506.0000],\n",
       "        [  22.6250,  466.2500, -425.5000,  ..., -591.0000,  388.0000,\n",
       "         -376.2500]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 @ weight.T + 4.0 * (X2 @ A2) @ B2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"jondurbin/airoboros-7b-gpt4-1.2-peft\", adapter_name=\"lora1\")\n",
    "model = PeftModel.from_pretrained(model.base_model.model, \"jondurbin/airoboros-7b-gpt4-1.2-peft\", adapter_name=\"lora2\")\n",
    "# model = PeftModel.from_pretrained(model.base_model.model, \"trl-lib/llama-7b-se-rl-peft\", adapter_name=\"se-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list(model.base_model.model.model.layers[0].self_attn.q_proj.named_modules())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, peft.tuners.lora.Linear):\n",
    "        module\n",
    "        break\n",
    "        # Do whatever operation you need to perform here\n",
    "        # For example, if you want to add lora_A and lora_B to the parameter, you can do:\n",
    "        # parameter.data = parameter.data + self.lora_A + self.lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_a_merged = torch.nn.Linear(module.lora_A['lora1'].in_features, module.lora_A['lora1'].out_features)\n",
    "# module.lora_A['merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import Linear\n",
    "\n",
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer(\"The LLaMA language model is\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=torch.ones_like(batch[\"input_ids\"]),\n",
    "        max_length=200,\n",
    "    )\n",
    "print(tokenizer.decode(out1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 10\n",
    "ctx_dim = 4096\n",
    "rank = 8\n",
    "scaling = 4.0\n",
    "fan_in_fan_out = False\n",
    "bias = None\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[286], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# blora\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([bs, seq_len, ctx_dim], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn([bs, seq_len, ctx_dim], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(([ctx_dim, ctx_dim]), device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bs' is not defined"
     ]
    }
   ],
   "source": [
    "# blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim], device=device)\n",
    "x1 = torch.randn([bs, seq_len, ctx_dim], device=device)\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]), device=device)\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False, device=device)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False, device=device)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False, device=device)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False, device=device)\n",
    "\n",
    "loraa = torch.cat([lora_a0.weight, lora_a1.weight], dim=1)\n",
    "lorab = torch.cat([lora_b0.weight, lora_b1.weight], dim=0)\n",
    "\n",
    "# forward pass\n",
    "def lora_forward(x, weight, lora1, lora2, scaling):\n",
    "    result = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "\n",
    "    if isinstance(lora1, torch.nn.Linear):\n",
    "        x = x.to(lora1.weight.dtype)\n",
    "        result += scaling * lora2(lora1(x))\n",
    "        return result\n",
    "    else:\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(lora1.dtype)\n",
    "        out = F.linear(x, transpose(lora1, fan_in_fan_out), bias=bias)\n",
    "        out = scaling * F.linear(out, transpose(lora2, fan_in_fan_out), bias=bias)\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        result += out\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "r0 = lora_forward(x0, weight, lora_a0, lora_b0, scaling)\n",
    "r1 = lora_forward(x1, weight, lora_a1, lora_b1, scaling)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora1: list,\n",
    "        lora2: list,\n",
    "        weight: torch.Tensor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lora_a = torch.nn.Parameter(torch.cat([lora1[0].weight, lora2[0].weight], dim=1))\n",
    "        self.lora_b = torch.nn.Parameter(torch.cat([lora1[1].weight, lora2[1].weight], dim=0))\n",
    "        self.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = F.linear(x, transpose(self.weight, fan_in_fan_out), bias=bias)\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(self.lora_a.dtype)\n",
    "\n",
    "        out = F.linear(x, transpose(self.lora_a, fan_in_fan_out), bias=bias)\n",
    "        out = scaling * F.linear(out, transpose(self.lora_b, fan_in_fan_out), bias=bias)\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        result += out\n",
    "        return result\n",
    "    \n",
    "blora = BLora(lora1=[lora_a0, lora_b0], lora2=[lora_a1, lora_b1], weight=weight)\n",
    "\n",
    "start = time.time()\n",
    "r2 = blora(x0)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora1 = torch.nn.Linear(in_features=bs * ctx_dim, out_features=rank, bias=False)\n",
    "lora2 = torch.nn.Linear(in_features=rank, out_features=bs * ctx_dim, bias=False)\n",
    "\n",
    "lora1.weight = torch.nn.Parameter(torch.cat([lora_a0.weight, lora_a1.weight], dim=1))\n",
    "lora2.weight = torch.nn.Parameter(torch.cat([lora_b0.weight, lora_b1.weight], dim=0))\n",
    "\n",
    "# forward pass\n",
    "result1 = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "x1 = x0.reshape(seq_len, -1)\n",
    "x1 = x1.to(lora1.weight.dtype)\n",
    "out1 = lora2(lora1(x1)) * scaling\n",
    "out1 = out1.reshape(bs, seq_len, -1)\n",
    "result1 += out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_model.bin\"\n",
    "peft_model_state_dict = torch.load(path)\n",
    "\n",
    "peft_config_path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_config.json\"\n",
    "peft_config = PeftConfig.from_json_file(peft_config_path)\n",
    "peft_config = {'default' : peft_config}\n",
    "model.peft_config = peft_config\n",
    "set_peft_model_state_dict(model, peft_model_state_dict, adapter_name=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
