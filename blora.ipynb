{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minimal-llama/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig, set_peft_model_state_dict\n",
    "import peft\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:10<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.view vs torch.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, l, c = 2, 10, 1024\n",
    "r = 8\n",
    "\n",
    "X1 = torch.randn(l, c)\n",
    "X2 = torch.randn(l, c)\n",
    "X = torch.cat([X1, X2], dim=0)\n",
    "XX = torch.cat([X1, X2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = torch.randn(c, r)\n",
    "A2 = torch.randn(c, r)\n",
    "A = torch.cat([A1, A2], dim=1)\n",
    "Y = X @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = torch.cat([torch.cat([A1, torch.zeros_like(A1)]), torch.cat([torch.zeros_like(A2), A2])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(XX @ AA)[:,r:] - X2 @ A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "Y1 = Y[:l, :r]\n",
    "Y2 = Y[l:, r:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0039, 0.0000, 0.0000, 0.0156, 0.0000, 0.0000, 0.0312],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0156, 0.0156],\n",
       "        [0.0000, 0.0000, 0.0024, 0.0000, 0.0039, 0.0078, 0.0078, 0.0000],\n",
       "        [0.0000, 0.0078, 0.0000, 0.0078, 0.0078, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0098, 0.0000, 0.0156, 0.0312, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0156, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0156, 0.0000, 0.0000, 0.0156, 0.0000],\n",
       "        [0.0078, 0.0000, 0.0039, 0.0156, 0.0000, 0.0000, 0.0020, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0068, 0.0078, 0.0000, 0.0000, 0.0000, 0.0312],\n",
       "        [0.0000, 0.0000, 0.0039, 0.0039, 0.0000, 0.0000, 0.0000, 0.0117]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y1 - X1 @ A1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"jondurbin/airoboros-7b-gpt4-1.2-peft\", adapter_name=\"lora1\")\n",
    "model = PeftModel.from_pretrained(model.base_model.model, \"jondurbin/airoboros-7b-gpt4-1.2-peft\", adapter_name=\"lora2\")\n",
    "# model = PeftModel.from_pretrained(model.base_model.model, \"trl-lib/llama-7b-se-rl-peft\", adapter_name=\"se-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list(model.base_model.model.model.layers[0].self_attn.q_proj.named_modules())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, peft.tuners.lora.Linear):\n",
    "        module\n",
    "        break\n",
    "        # Do whatever operation you need to perform here\n",
    "        # For example, if you want to add lora_A and lora_B to the parameter, you can do:\n",
    "        # parameter.data = parameter.data + self.lora_A + self.lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_a_merged = torch.nn.Linear(module.lora_A['lora1'].in_features, module.lora_A['lora1'].out_features)\n",
    "# module.lora_A['merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import Linear\n",
    "\n",
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer(\"The LLaMA language model is\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=torch.ones_like(batch[\"input_ids\"]),\n",
    "        max_length=200,\n",
    "    )\n",
    "print(tokenizer.decode(out1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 10\n",
    "ctx_dim = 4096\n",
    "rank = 8\n",
    "scaling = 4.0\n",
    "fan_in_fan_out = False\n",
    "bias = None\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim], device=device)\n",
    "x1 = torch.randn([bs, seq_len, ctx_dim], device=device)\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]), device=device)\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False, device=device)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False, device=device)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False, device=device)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False, device=device)\n",
    "\n",
    "loraa = torch.cat([lora_a0.weight, lora_a1.weight], dim=1)\n",
    "lorab = torch.cat([lora_b0.weight, lora_b1.weight], dim=0)\n",
    "\n",
    "# forward pass\n",
    "def lora_forward(x, weight, lora1, lora2, scaling):\n",
    "    result = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "\n",
    "    if isinstance(lora1, torch.nn.Linear):\n",
    "        x = x.to(lora1.weight.dtype)\n",
    "        result += scaling * lora2(lora1(x))\n",
    "        return result\n",
    "    else:\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(lora1.dtype)\n",
    "        out = F.linear(x, transpose(lora1, fan_in_fan_out), bias=bias)\n",
    "        out = scaling * F.linear(out, transpose(lora2, fan_in_fan_out), bias=bias)\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        result += out\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "r0 = lora_forward(x0, weight, lora_a0, lora_b0, scaling)\n",
    "r1 = lora_forward(x1, weight, lora_a1, lora_b1, scaling)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_a0.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_b0.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora1: list,\n",
    "        lora2: list,\n",
    "        weight: torch.Tensor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lora_a = torch.nn.Parameter(torch.cat([lora1[0].weight, lora2[0].weight], dim=1))\n",
    "        self.lora_b = torch.nn.Parameter(torch.cat([lora1[1].weight, lora2[1].weight], dim=0))\n",
    "        self.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = F.linear(x, transpose(self.weight, fan_in_fan_out), bias=bias)\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(self.lora_a.dtype)\n",
    "\n",
    "        out = F.linear(x, transpose(self.lora_a, fan_in_fan_out), bias=bias)\n",
    "        out = scaling * F.linear(out, transpose(self.lora_b, fan_in_fan_out), bias=bias)\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        result += out\n",
    "        return result\n",
    "    \n",
    "blora = BLora(lora1=[lora_a0, lora_b0], lora2=[lora_a1, lora_b1], weight=weight)\n",
    "\n",
    "start = time.time()\n",
    "r2 = blora(x0)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora1 = torch.nn.Linear(in_features=bs * ctx_dim, out_features=rank, bias=False)\n",
    "lora2 = torch.nn.Linear(in_features=rank, out_features=bs * ctx_dim, bias=False)\n",
    "\n",
    "lora1.weight = torch.nn.Parameter(torch.cat([lora_a0.weight, lora_a1.weight], dim=1))\n",
    "lora2.weight = torch.nn.Parameter(torch.cat([lora_b0.weight, lora_b1.weight], dim=0))\n",
    "\n",
    "# forward pass\n",
    "result1 = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "x1 = x0.reshape(seq_len, -1)\n",
    "x1 = x1.to(lora1.weight.dtype)\n",
    "out1 = lora2(lora1(x1)) * scaling\n",
    "out1 = out1.reshape(bs, seq_len, -1)\n",
    "result1 += out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_model.bin\"\n",
    "peft_model_state_dict = torch.load(path)\n",
    "\n",
    "peft_config_path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_config.json\"\n",
    "peft_config = PeftConfig.from_json_file(peft_config_path)\n",
    "peft_config = {'default' : peft_config}\n",
    "model.peft_config = peft_config\n",
    "set_peft_model_state_dict(model, peft_model_state_dict, adapter_name=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
