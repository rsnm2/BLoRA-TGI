{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minimal-llama/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import TextStreamer\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftConfig, set_peft_model_state_dict, PeftModel\n",
    "import peft\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import inspect\n",
    "from functools import wraps\n",
    "\n",
    "from blora_utils import forward, BatchStreamer, StreamingPeftModel\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "Linear.forward = forward\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loras = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", \"winddude/wizardLM-LlaMA-LoRA-7B\"]\n",
    "adapters = [lora.replace(\".\", \"_\") for lora in loras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StreamingPeftModel.from_pretrained(model, loras[0], adapter_name=adapters[0])\n",
    "for lora, adapter in zip(loras[1:], adapters[1:]):\n",
    "    model = StreamingPeftModel.from_pretrained(model.base_model.model, lora, adapter_name=adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer([\"The LLaMA language model is\"] * 3, return_tensors=\"pt\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    module.batch_lora_ids = adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['stream_output'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/minimal-llama/blora.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m rep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repetitions):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     starter\u001b[39m.\u001b[39mrecord()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mbatch[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         stream_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     ):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         batch_decoded \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(torch\u001b[39m.\u001b[39mcat([out\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outputs], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/minimal-llama/venv/lib/python3.10/site-packages/peft/peft_model.py:977\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m    976\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    978\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/minimal-llama/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/minimal-llama/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1271\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m-> 1271\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1273\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/minimal-llama/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1144\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1145\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1146\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1147\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['stream_output'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 1\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        for out in model.generate(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            max_length=200,\n",
    "            stream_output=True\n",
    "        ):\n",
    "            outputs.append(out)\n",
    "            batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\n\\n\".join([lora + \":\\n\" + decoded for lora, decoded in zip(loras, batch_decoded)]))\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn / 1000, std_syn / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 4, 8, 16, 32, 64])\n",
    "y1 = np.array([14.13, 16.05, 22.43, 35.18, 62.25, 114.11, 219.20])\n",
    "y2 = 14.13 * x\n",
    "\n",
    "plt.plot(x, y1, 'b-')\n",
    "plt.plot(x, y2, 'r--')\n",
    "\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (s)')\n",
    "\n",
    "plt.legend(['Batched Lora', 'Sequential'], loc='upper left')\n",
    "plt.title('Generating 200 tokens with Llama-7B using Batched Lora vs Sequential on A100-80gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get slope of y1\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y1)\n",
    "print(slope)\n",
    "print(intercept)\n",
    "\n",
    "# which lib to import starts from"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
