{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minimal-llama/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig, set_peft_model_state_dict\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"jondurbin/airoboros-7b-gpt4-1.2-peft\", adapter_name=\"airoboros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model.base_model.model, \"trl-lib/llama-7b-se-rl-peft\", adapter_name=\"se-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (airoboros): Dropout(p=0.05, inplace=False)\n",
       "                  (se-rl): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (se-rl): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (se-rl): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (airoboros): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (airoboros): Dropout(p=0.05, inplace=False)\n",
       "                  (se-rl): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (se-rl): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (se-rl): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (airoboros): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (airoboros): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (airoboros): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): Linear(\n",
       "                in_features=11008, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (airoboros): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (airoboros): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer(\"The LLaMA language model is\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=torch.ones_like(batch[\"input_ids\"]),\n",
    "        max_length=200,\n",
    "    )\n",
    "print(tokenizer.decode(out1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 10\n",
    "ctx_dim = 4096\n",
    "rank = 8\n",
    "scaling = 4.0\n",
    "fan_in_fan_out = False\n",
    "bias = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora1 = torch.cat([lora_a0.weight, lora_a1.weight], dim=1)\n",
    "lora2 = torch.cat([lora_b0.weight, lora_b1.weight], dim=0)\n",
    "\n",
    "# forward pass\n",
    "result = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "x = x0.reshape(seq_len, -1)\n",
    "x = x.to(lora1.dtype)\n",
    "\n",
    "out = F.linear(x, transpose(lora1, fan_in_fan_out), bias=bias)\n",
    "out = scaling * F.linear(out, transpose(lora2, fan_in_fan_out), bias=bias)\n",
    "out = out.reshape(bs, seq_len, -1)\n",
    "result += out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora1: list,\n",
    "        lora2: list,\n",
    "        weight: torch.Tensor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lora_a = torch.nn.Parameter(torch.cat([lora1[0].weight, lora2[0].weight], dim=1))\n",
    "        self.lora_b = torch.nn.Parameter(torch.cat([lora1[1].weight, lora2[1].weight], dim=0))\n",
    "        self.weight = torch.nn.Parameter(weight)\n",
    "        print(f\"lora_a: {self.lora_a.shape}, lora_b: {self.lora_b.shape}, weight: {self.weight.shape}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        print(f\"x: {x.shape}\")\n",
    "        result = F.linear(x, transpose(self.weight, fan_in_fan_out), bias=bias)\n",
    "        print(f\"result: {result.shape}\")\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(self.lora_a.dtype)\n",
    "        print(f\"x: {x.shape}\")\n",
    "\n",
    "        out = F.linear(x, transpose(self.lora_a, fan_in_fan_out), bias=bias)\n",
    "        print(f\"out: {out.shape}\")\n",
    "        out = scaling * F.linear(out, transpose(self.lora_b, fan_in_fan_out), bias=bias)\n",
    "        print(f\"out: {out.shape}\")\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        print(f\"out: {out.shape}\")\n",
    "        result += out\n",
    "        return result\n",
    "    \n",
    "blora = BLora(lora1=[lora_a0, lora_b0], lora2=[lora_a1, lora_b1], weight=weight)\n",
    "result2 = blora(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora1 = torch.nn.Linear(in_features=bs * ctx_dim, out_features=rank, bias=False)\n",
    "lora2 = torch.nn.Linear(in_features=rank, out_features=bs * ctx_dim, bias=False)\n",
    "\n",
    "lora1.weight = torch.nn.Parameter(torch.cat([lora_a0.weight, lora_a1.weight], dim=1))\n",
    "lora2.weight = torch.nn.Parameter(torch.cat([lora_b0.weight, lora_b1.weight], dim=0))\n",
    "\n",
    "# forward pass\n",
    "result1 = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "x1 = x0.reshape(seq_len, -1)\n",
    "x1 = x1.to(lora1.weight.dtype)\n",
    "out1 = lora2(lora1(x1)) * scaling\n",
    "out1 = out1.reshape(bs, seq_len, -1)\n",
    "result1 += out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_model.bin\"\n",
    "peft_model_state_dict = torch.load(path)\n",
    "\n",
    "peft_config_path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_config.json\"\n",
    "peft_config = PeftConfig.from_json_file(peft_config_path)\n",
    "peft_config = {'default' : peft_config}\n",
    "model.peft_config = peft_config\n",
    "set_peft_model_state_dict(model, peft_model_state_dict, adapter_name=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
