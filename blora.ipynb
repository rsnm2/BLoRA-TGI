{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rshaw/BLoRA-TGI/tgi\n"
     ]
    }
   ],
   "source": [
    "%cd tgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rshaw/BLoRA-TGI/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from service.blora_utils import load_loras, prepare_batch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# from peft.tuners.lora import Linear\n",
    "# from blora_utils import forward\n",
    "# Linear.forward = forward\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.86it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_path = \"decapoda-research/llama-7b-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loras = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", \"winddude/wizardLM-LlaMA-LoRA-7B\"]\n",
    "model, lora_map = load_loras(model, loras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [('Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.',\n",
    "  'jondurbin/airoboros-7b-gpt4-1.2-peft'),\n",
    "#  ('Write a 6 line dialogue between a character and a magical creature that only they can see.',\n",
    "#   'trl-lib/llama-7b-se-rl-peft'),\n",
    "#  ('Describe a four sentence scene where a character discovers a hidden talent that changes their life forever.',\n",
    "#   'winddude/wizardLM-LlaMA-LoRA-7B'),\n",
    "#  ('Sculpt a three verse poem about the feeling of walking through a lush, vibrant garden in full bloom.',\n",
    "#   'trl-lib/llama-7b-se-rl-peft'),\n",
    "#  ('Develop an eight sentence short story about a character who can bring their dreams into reality, but only for a limited time.',\n",
    "#   'winddude/wizardLM-LlaMA-LoRA-7B')\n",
    "]\n",
    "\n",
    "batch_lora_ids = [inp[1] for inp in inputs]\n",
    "\n",
    "batch = prepare_batch(inputs, tokenizer, model, lora_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jondurbin/airoboros-7b-gpt4-1.2-peft:\n",
      "Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.\n",
      "\n",
      "The character, who is a young boy named Timmy, stumbles upon a secret room in his house that contained relics from his future. The room was hidden behind a bookcase in the library, and it was filled with strange artifacts and documents.\n",
      "Timmy's curiosity got the best of him, and he decided\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for out in model.generate(\n",
    "    **batch,\n",
    "    max_length=100,\n",
    "    stream_output=True\n",
    "):\n",
    "    outputs.append(out)\n",
    "    batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))\n",
    "    clear_output(wait=True)\n",
    "    print(\"\\n\\n\".join([lora + \":\\n\" + prompt + '\\n' + decoded for (prompt, lora), decoded in zip(inputs, batch_decoded)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4451, 1220, 263, 5320, 10541, 3273, 5828, 988, 263, 2931, 380, 3774, 793, 2501, 263, 7035, 5716, 297, 1009, 3699, 393, 3743, 337, 506, 29879, 515, 1009, 5434, 29889]\n",
      "tensor([[    0,  4451,  1220,   263,  5320, 10541,  3273,  5828,   988,   263,\n",
      "          2931,   380,  3774,   793,  2501,   263,  7035,  5716,   297,  1009,\n",
      "          3699,   393,  3743,   337,   506, 29879,   515,  1009,  5434, 29889]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4451, 1220, 263, 5320, 10541, 3273, 5828, 988, 263, 2931, 380, 3774, 793, 2501, 263, 7035, 5716, 297, 1009, 3699, 393, 3743, 337, 506, 29879, 515, 1009, 5434, 29889]\n",
      "input_ids: tensor([[    0,  4451,  1220,   263,  5320, 10541,  3273,  5828,   988,   263,\n",
      "          2931,   380,  3774,   793,  2501,   263,  7035,  5716,   297,  1009,\n",
      "          3699,   393,  3743,   337,   506, 29879,   515,  1009,  5434, 29889]])\n",
      "position_ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]])\n",
      "use_cache: True\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])\n",
      "13\n",
      "\n",
      "\n",
      "\n",
      "input_ids: tensor([[13]])\n",
      "position_ids: tensor([[30]])\n",
      "use_cache: True\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "1576\n",
      "\n",
      "\n",
      "\n",
      "input_ids: tensor([[1576]])\n",
      "position_ids: tensor([[31]])\n",
      "use_cache: True\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "2931\n",
      "\n",
      "\n",
      "\n",
      "input_ids: tensor([[2931]])\n",
      "position_ids: tensor([[32]])\n",
      "use_cache: True\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "29892\n",
      "\n",
      "\n",
      "\n",
      "input_ids: tensor([[29892]])\n",
      "position_ids: tensor([[33]])\n",
      "use_cache: True\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "1058\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = batch.input_ids[0,:].tolist()\n",
    "print(tokens)\n",
    "\n",
    "model_kwargs = {\n",
    "    \"attention_mask\": batch.attention_mask,\n",
    "    \"use_cache\": True\n",
    "}\n",
    "\n",
    "input_ids = batch.input_ids\n",
    "\n",
    "for _ in range(5):\n",
    "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "    for key in model_inputs:\n",
    "        if key != \"past_key_values\":\n",
    "            print(f\"{key}: {model_inputs[key]}\")\n",
    "    \n",
    "    outputs = model(\n",
    "        **model_inputs,\n",
    "        return_dict=True,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "    tokens.append(next_tokens.item())\n",
    "\n",
    "    input_ids = torch.cat([batch.input_ids, next_tokens[:, None]], dim=-1)\n",
    "    model_kwargs = model._update_model_kwargs_for_generation(\n",
    "        outputs, model_kwargs, is_encoder_decoder=False\n",
    "    )\n",
    "\n",
    "    print(next_tokens.item())\n",
    "    print(\"\\n\\n\")\n",
    "    # print(len(tokens))\n",
    "    # print(tokenizer.decode(tokens[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "2\n",
      "torch.Size([1, 32, 30, 128])\n",
      "tensor(-0.0251)\n"
     ]
    }
   ],
   "source": [
    "print(len(model_kwargs[\"past_key_values\"]))\n",
    "print(len(model_kwargs[\"past_key_values\"][0]))\n",
    "print(model_kwargs[\"past_key_values\"][0][0].shape)\n",
    "print(model_kwargs[\"past_key_values\"][0][0][0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  4451,  1220,   263,  5320, 10541,  3273,  5828,   988,   263,\n",
       "          2931,   380,  3774,   793,  2501,   263,  7035,  5716,   297,  1009,\n",
       "          3699,   393,  3743,   337,   506, 29879,   515,  1009,  5434, 29889,\n",
       "          1576]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch.input_ids\n",
    "attention_mask = batch.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StreamingPeftModel(\n",
       "  (base_model): BLoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): BLinear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Dropout(p=0.05, inplace=False)\n",
       "                  (trl-lib/llama-7b-se-rl-peft): Dropout(p=0.05, inplace=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (trl-lib/llama-7b-se-rl-peft): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (trl-lib/llama-7b-se-rl-peft): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): BLinear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Dropout(p=0.05, inplace=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): BLinear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Dropout(p=0.05, inplace=False)\n",
       "                  (trl-lib/llama-7b-se-rl-peft): Dropout(p=0.05, inplace=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (trl-lib/llama-7b-se-rl-peft): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (trl-lib/llama-7b-se-rl-peft): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): BLinear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Dropout(p=0.05, inplace=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (winddude/wizardLM-LlaMA-LoRA-7B): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): BLinear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): BLinear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): BLinear(\n",
       "                in_features=11008, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (jondurbin/airoboros-7b-gpt4-1_2-peft): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
