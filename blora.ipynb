{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import TextStreamer\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftConfig, set_peft_model_state_dict\n",
    "import peft\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import inspect\n",
    "from functools import wraps\n",
    "\n",
    "from blora import forward, BatchStreamer, StreamingPeftModel\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "Linear.forward = forward\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loras = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", \"winddude/wizardLM-LlaMA-LoRA-7B\"]\n",
    "adapters = [lora.replace(\".\", \"_\") for lora in loras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StreamingPeftModel.from_pretrained(model, loras[0], adapter_name=adapters[0])\n",
    "for lora, adapter in zip(loras[1:], adapters[1:]):\n",
    "    model = StreamingPeftModel.from_pretrained(model.base_model.model, lora, adapter_name=adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer([\"The LLaMA language model is\"] * 3, return_tensors=\"pt\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    module.batch_lora_ids = adapters\n",
    "\n",
    "streamer = BatchStreamer(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jondurbin/airoboros-7b-gpt4-1.2-peft:\n",
      "a large-scale neural network trained on a large corpus of text. It can be used for various tasks such as text classification, text generation, and natural language understanding.\n",
      "\n",
      "## Architecture\n",
      "\n",
      "The LLaMA model consists of an encoder-decoder architecture with attention mechanism. The encoder part is a bidirectional Gated Recurrent Unit (GRU) network, while the decoder part is a simple feed-forward network. The attention mechanism is used to focus on important parts of the input sequence during decoding.\n",
      "\n",
      "## Training\n",
      "\n",
      "The LLaMA model is trained using the negative sampling technique. In this approach, the model is trained on a large corpus of text and then used to generate new sentences. To generate new sentences, the model first generates a set of candidate sentences using the encoder part. Then, it selects the best candidate sentence based on its probability score. The probability score is calculated by\n",
      "\n",
      "trl-lib/llama-7b-se-rl-peft:\n",
      "a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLa\n",
      "\n",
      "winddude/wizardLM-LlaMA-LoRA-7B:\n",
      "a neural network-based language model that is trained on a large corpus of text. It is designed to be used in a variety of applications, including machine translation, speech recognition, and text classification. The LLaMA language model is implemented in Python and can be used in a variety of applications.\n",
      "The LLaMA language model is a neural network-based language model that is trained on a large corpus of text. It is designed to be used in a variety of applications, including machine translation, speech recognition, and text classification. The LLaMA language model is implemented in Python and can be used in a variety of applications.\n",
      "The LLaMA language model is a neural network-based language model that is trained on a large corpus of text. It is designed to be used in a variety of applications, including machine translation, speech recognition, and text classification. The LLaMA language model is implemented in Python and can be\n",
      "16.746583984375 0.0\n"
     ]
    }
   ],
   "source": [
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 1\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        for out in model.generate(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            max_length=200,\n",
    "        ):\n",
    "            outputs.append(out)\n",
    "            batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\n\\n\".join([lora + \":\\n\" + decoded for lora, decoded in zip(loras, batch_decoded)]))\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn / 1000, std_syn / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tokenizer.batch_decode(out)\n",
    "\n",
    "for lora, text in zip(loras, texts):\n",
    "    print(lora + ':')\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 4, 8, 16, 32, 64])\n",
    "y1 = np.array([14.13, 16.05, 22.43, 35.18, 62.25, 114.11, 219.20])\n",
    "y2 = 14.13 * x\n",
    "\n",
    "plt.plot(x, y1, 'b-')\n",
    "plt.plot(x, y2, 'r--')\n",
    "\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (s)')\n",
    "\n",
    "plt.legend(['Batched Lora', 'Sequential'], loc='upper left')\n",
    "plt.title('Generating 200 tokens with Llama-7B using Batched Lora vs Sequential on A100-80gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get slope of y1\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y1)\n",
    "print(slope)\n",
    "print(intercept)\n",
    "\n",
    "# which lib to import starts from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.7 + 5 * 3.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
