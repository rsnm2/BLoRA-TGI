{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig, set_peft_model_state_dict\n",
    "import peft\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"jondurbin/airoboros-7b-gpt4-1.2-peft\", adapter_name=\"airoboros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model.base_model.model, \"trl-lib/llama-7b-se-rl-peft\", adapter_name=\"se-rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  in_features=4096, out_features=4096, bias=False\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (airoboros): Dropout(p=0.05, inplace=False)\n",
       "    (se-rl): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict(\n",
       "    (airoboros): Linear(in_features=4096, out_features=64, bias=False)\n",
       "    (se-rl): Linear(in_features=4096, out_features=16, bias=False)\n",
       "  )\n",
       "  (lora_B): ModuleDict(\n",
       "    (airoboros): Linear(in_features=64, out_features=4096, bias=False)\n",
       "    (se-rl): Linear(in_features=16, out_features=4096, bias=False)\n",
       "  )\n",
       "  (lora_embedding_A): ParameterDict()\n",
       "  (lora_embedding_B): ParameterDict()\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.tuners.lora.Linear"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(model.base_model.model.model.layers[0].self_attn.q_proj.named_modules())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, peft.tuners.lora.Linear):\n",
    "        module\n",
    "        break\n",
    "        # Do whatever operation you need to perform here\n",
    "        # For example, if you want to add lora_A and lora_B to the parameter, you can do:\n",
    "        # parameter.data = parameter.data + self.lora_A + self.lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=64, bias=False)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.lora_A['airoboros']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = torch.nn.Linear()\n",
    "module.lora_A['merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import Linear\n",
    "\n",
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  The LLaMA language model is a large-scale neural network trained on a large corpus of text. It can be used for various tasks such as text classification, text generation, and natural language understanding.\n",
      "\n",
      "## Architecture\n",
      "\n",
      "The LLaMA model consists of an encoder-decoder architecture with attention mechanism. The encoder part is a bidirectional Gated Recurrent Unit (GRU) network, while the decoder part is a simple feed-forward network. The attention mechanism is used to focus on important parts of the input sequence during decoding.\n",
      "\n",
      "## Training\n",
      "\n",
      "The LLaMA model is trained using the negative sampling technique. In this approach, the model is trained on a large corpus of text and then used to generate new sentences. To generate new sentences, the model first generates a set of candidate sentences using the encoder part. Then, it selects the best candidate sentence based on its probability score. The probability score is calculated by\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer(\"The LLaMA language model is\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=torch.ones_like(batch[\"input_ids\"]),\n",
    "        max_length=200,\n",
    "    )\n",
    "print(tokenizer.decode(out1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 10\n",
    "ctx_dim = 4096\n",
    "rank = 8\n",
    "scaling = 4.0\n",
    "fan_in_fan_out = False\n",
    "bias = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_forward: 424.1466522216797 microseceonds\n"
     ]
    }
   ],
   "source": [
    "# blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora1 = torch.cat([lora_a0.weight, lora_a1.weight], dim=1)\n",
    "lora2 = torch.cat([lora_b0.weight, lora_b1.weight], dim=0)\n",
    "\n",
    "# forward pass\n",
    "def lora_forward(x0, weight, lora1, lora2):\n",
    "    result = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "    x = x0.reshape(seq_len, -1)\n",
    "    x = x.to(lora1.dtype)\n",
    "\n",
    "    out = F.linear(x, transpose(lora1, fan_in_fan_out), bias=bias)\n",
    "    out = scaling * F.linear(out, transpose(lora2, fan_in_fan_out), bias=bias)\n",
    "    out = out.reshape(bs, seq_len, -1)\n",
    "    result += out\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "lora_forward(x0, weight, lora1, lora2)\n",
    "lora_forward(x0, weight, lora1, lora2)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_forward: 279.1881561279297 microseceonds\n"
     ]
    }
   ],
   "source": [
    "class BLora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora1: list,\n",
    "        lora2: list,\n",
    "        weight: torch.Tensor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lora_a = torch.nn.Parameter(torch.cat([lora1[0].weight, lora2[0].weight], dim=1))\n",
    "        self.lora_b = torch.nn.Parameter(torch.cat([lora1[1].weight, lora2[1].weight], dim=0))\n",
    "        self.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = F.linear(x, transpose(self.weight, fan_in_fan_out), bias=bias)\n",
    "        x = x.reshape(seq_len, -1)\n",
    "        x = x.to(self.lora_a.dtype)\n",
    "\n",
    "        out = F.linear(x, transpose(self.lora_a, fan_in_fan_out), bias=bias)\n",
    "        out = scaling * F.linear(out, transpose(self.lora_b, fan_in_fan_out), bias=bias)\n",
    "        out = out.reshape(bs, seq_len, -1)\n",
    "        result += out\n",
    "        return result\n",
    "    \n",
    "blora = BLora(lora1=[lora_a0, lora_b0], lora2=[lora_a1, lora_b1], weight=weight)\n",
    "\n",
    "start = time.time()\n",
    "blora(x0)\n",
    "print(f\"lora_forward: {(time.time() - start)*1e6} microseceonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated blora\n",
    "x0 = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank, bias=False)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim, bias=False)\n",
    "\n",
    "lora1 = torch.nn.Linear(in_features=bs * ctx_dim, out_features=rank, bias=False)\n",
    "lora2 = torch.nn.Linear(in_features=rank, out_features=bs * ctx_dim, bias=False)\n",
    "\n",
    "lora1.weight = torch.nn.Parameter(torch.cat([lora_a0.weight, lora_a1.weight], dim=1))\n",
    "lora2.weight = torch.nn.Parameter(torch.cat([lora_b0.weight, lora_b1.weight], dim=0))\n",
    "\n",
    "# forward pass\n",
    "result1 = F.linear(x0, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "x1 = x0.reshape(seq_len, -1)\n",
    "x1 = x1.to(lora1.weight.dtype)\n",
    "out1 = lora2(lora1(x1)) * scaling\n",
    "out1 = out1.reshape(bs, seq_len, -1)\n",
    "result1 += out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_model.bin\"\n",
    "peft_model_state_dict = torch.load(path)\n",
    "\n",
    "peft_config_path = \"/home/ubuntu/airoboros-7b-gpt4-1.2-peft/adapter_config.json\"\n",
    "peft_config = PeftConfig.from_json_file(peft_config_path)\n",
    "peft_config = {'default' : peft_config}\n",
    "model.peft_config = peft_config\n",
    "set_peft_model_state_dict(model, peft_model_state_dict, adapter_name=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
