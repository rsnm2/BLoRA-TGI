{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/minimal-llama/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from finetune_peft import get_peft_config, PEFTArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "tokenizer_path = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:09<00:00,  3.30it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLaMA language model is a large-scale language model that is trained on a large corpus of text. The LLaMA language model is a large-scale language model that is trained on a large corpus of text.\n",
      "The LLa\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "batch = tokenizer(\"The LLaMA language model is\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=torch.ones_like(batch[\"input_ids\"]),\n",
    "        max_length=200,\n",
    "    )\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAttention(\n",
       "  (q_proj): Linear(\n",
       "    in_features=4096, out_features=4096, bias=False\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "  )\n",
       "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (v_proj): Linear(\n",
       "    in_features=4096, out_features=4096, bias=False\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "  )\n",
       "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.layers[0].self_attn #.q_proj.lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 8\n",
    "ctx_dim = 4096\n",
    "rank = 8\n",
    "scaling = 4.0\n",
    "fan_in_fan_out = False\n",
    "bias = None\n",
    "\n",
    "x = torch.randn([bs, seq_len, ctx_dim])\n",
    "weight = torch.randn(([ctx_dim, ctx_dim]))\n",
    "weight2x = torch.cat([weight, weight], dim=1)\n",
    "\n",
    "lora_a0 = torch.nn.Linear(in_features=ctx_dim, out_features=rank)\n",
    "lora_b0 = torch.nn.Linear(in_features=rank, out_features=ctx_dim)\n",
    "\n",
    "lora_a1 = torch.nn.Linear(in_features=ctx_dim, out_features=rank)\n",
    "lora_b1 = torch.nn.Linear(in_features=rank, out_features=ctx_dim)\n",
    "\n",
    "lora_a = torch.nn.Linear(in_features=bs * ctx_dim, out_features=rank, )\n",
    "lora_b = torch.nn.Linear(in_features=rank, out_features=bs * ctx_dim)\n",
    "\n",
    "lora_a.weight = torch.nn.Parameter(torch.cat([lora_a0.weight, lora_a1.weight], dim=1))\n",
    "lora_b.weight = torch.nn.Parameter(torch.cat([lora_b0.weight, lora_b1.weight], dim=0))\n",
    "\n",
    "# forward pass\n",
    "result = F.linear(x, transpose(weight, fan_in_fan_out), bias=bias)\n",
    "x = x.reshape(seq_len, -1)\n",
    "x = x.to(lora_a.weight.dtype)\n",
    "out = lora_b(lora_a(x)) * scaling\n",
    "out = out.reshape(bs, seq_len, -1)\n",
    "result += out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor):\n",
    "    previous_dtype = x.dtype\n",
    "    if self.active_adapter not in self.lora_A.keys():\n",
    "        return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    if self.disable_adapters:\n",
    "        if self.r[self.active_adapter] > 0 and self.merged:\n",
    "            self.unmerge()\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "    elif self.r[self.active_adapter] > 0 and not self.merged:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n",
    "\n",
    "        result += (\n",
    "            self.lora_B[self.active_adapter](\n",
    "                self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
    "            )\n",
    "            * self.scaling[self.active_adapter]\n",
    "        )\n",
    "    else:\n",
    "        result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
    "\n",
    "    result = result.to(previous_dtype)\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
