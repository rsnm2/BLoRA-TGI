{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BlinearBMM Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tuples = [\n",
    "    # ('Outline a five sentence short story about the Patriots',\n",
    "    # 'jondurbin/airoboros-7b-gpt4-1.2-peft', 100),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 100),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 100),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Request, GenerateParameters, Batch\n",
    "\n",
    "requests = [\n",
    "    Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generate_parameters=GenerateParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ) for idx, inp in enumerate(r_tuples)\n",
    "]\n",
    "\n",
    "batch = Batch(id=0, requests=requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7850e908ea544914984c66f065d3a302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89c7e39be024e53a301e645bf6c79ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from service.causal_lm import BLoraCausalLM, BLoraCausalLMBatch\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "# lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "lora_ids = [\"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "model = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids,\n",
    "    blinear_type=None\n",
    ")\n",
    "\n",
    "model_bmm = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids,\n",
    "    blinear_type=\"bmm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org/wiki/Selection_sort)\n",
      "\\item [Insertion sort](http://en.wikipedia.org/wiki/Insertion_sort)\n",
      "\\item [Merge sort](http://en.wikipedia.org/wiki/Merge_\n",
      "\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "\\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end{code}\n",
      "\n",
      "Comment: This is not the correct answer.\n",
      "\n",
      "Comment: @JonSkeet: I'm not sure what you mean.  The\n",
      "\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a wave of her hand, she was transported to a tropical island, where she spent the day exploring the lush jungle and swimming\n"
     ]
    }
   ],
   "source": [
    "tokens_lst = [[] for _ in range(len(r_tuples))]\n",
    "\n",
    "clm_batch = BLoraCausalLMBatch.from_batch(\n",
    "    batch=batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "for _ in range(100):\n",
    "    generations, clm_batch = model.generate_token(clm_batch)\n",
    "    for generation in generations:\n",
    "        tokens_lst[generation.request_id].append(generation.token_id)\n",
    "\n",
    "for tokens in tokens_lst:\n",
    "    print(model.tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org/wiki/Selection_sort)\n",
      "\\item [Insertion sort](http://en.wikipedia.org/wiki/Insertion_sort)\n",
      "\\item [Merge sort](http://en.wikipedia.org/wiki/Merge_\n",
      "\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "\\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end{code}\n",
      "\n",
      "Comment: This is the most efficient way to do it.\n",
      "\n",
      "Comment: @Jon Skeet: I'm not sure about that.\n",
      "\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a wave of her hand, she was transported to a tropical island, where she spent the day exploring the lush jungle and swimming\n"
     ]
    }
   ],
   "source": [
    "tokens_lst = [[] for _ in range(len(r_tuples))]\n",
    "\n",
    "clm_batch = BLoraCausalLMBatch.from_batch(\n",
    "    batch=batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "for _ in range(100):\n",
    "    generations, clm_batch = model_bmm.generate_token(clm_batch)\n",
    "    for generation in generations:\n",
    "        tokens_lst[generation.request_id].append(generation.token_id)\n",
    "\n",
    "for tokens in tokens_lst:\n",
    "    print(model.tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BLinearBMM Timing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tuples = [\n",
    "    # ('Outline a five sentence short story about the Patriots',\n",
    "    # 'jondurbin/airoboros-7b-gpt4-1.2-peft', 100),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 100),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 100),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100)\n",
    "] * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Request, GenerateParameters, Batch\n",
    "\n",
    "requests = [\n",
    "    Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generate_parameters=GenerateParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ) for idx, inp in enumerate(r_tuples)\n",
    "]\n",
    "\n",
    "batch = Batch(id=0, requests=requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4416546f35243ddbf141721471a0ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baba6cf1b8c24e47a4e90387ebdd8c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from service.causal_lm import BLoraCausalLM, BLoraCausalLMBatch\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "# lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "lora_ids = [\"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "model = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids,\n",
    "    blinear_type=\"timed\"\n",
    ")\n",
    "\n",
    "model_bmm = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids,\n",
    "    blinear_type=\"bmm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm_batch = BLoraCausalLMBatch.from_batch(\n",
    "    batch=batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = model.model.base_model.model.model.layers[1].self_attn.q_proj\n",
    "my_layer_bmm = model_bmm.model.base_model.model.model.layers[1].self_attn.q_proj\n",
    "\n",
    "batch_lora_ids = [model.lora_map[r[1]] for r in r_tuples]\n",
    "my_layer_bmm.set_batch_lora_ids(lora_ids=batch_lora_ids)\n",
    "\n",
    "my_layer.get_example = True\n",
    "my_layer.report_timing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 4096])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    _, clm_batch = model.generate_token(clm_batch)\n",
    "my_layer.x_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "lora_A_weights = torch.stack([my_layer.lora_A[lora_id].weight.T for lora_id in my_layer.batch_lora_ids])\n",
    "lora_B_weights = torch.stack([my_layer.lora_B[lora_id].weight.T for lora_id in my_layer.batch_lora_ids])\n",
    "scales = torch.tensor([my_layer.scaling[lora_id] for lora_id in my_layer.batch_lora_ids]).reshape(-1,1,1)\n",
    "\n",
    "x = my_layer.x_example\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_bmm = my_layer_bmm(x)\n",
    "    output_loop = my_layer(x)\n",
    "\n",
    "print(output_bmm - output_loop)\n",
    "print((output_bmm - output_loop).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([100, 1, 4096])\n",
      "\n",
      "New bmm-based time: 0.020\n",
      "Original time: 0.634\n",
      "Speedup: 31.0x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "ITERATIONS = 100\n",
    "\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(ITERATIONS):\n",
    "        output = my_layer_bmm(x)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    bmm_time = end - start\n",
    "    print(f\"New bmm-based time: {bmm_time:0.3f}\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _  in range(ITERATIONS):\n",
    "        output = my_layer(x)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    blora_time = end - start\n",
    "    print(f\"Original time: {blora_time:0.3f}\")\n",
    "\n",
    "    print(f\"Speedup: {blora_time / bmm_time:0.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BLinearTimed**\n",
    "\n",
    "Swap `BLinear` for `BLinearTimed` in `blora_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tuples = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 100),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 100),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 100),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Request, GenerateParameters, Batch\n",
    "\n",
    "requests = [\n",
    "    Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generate_parameters=GenerateParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ) for idx, inp in enumerate(r_tuples)\n",
    "] * 20\n",
    "\n",
    "batch = Batch(id=0, requests=requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.causal_lm import BLoraCausalLM, BLoraCausalLMBatch\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "model = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm_batch = BLoraCausalLMBatch.from_batch(\n",
    "    batch=batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = model.model.base_model.model.model.layers[1].self_attn.q_proj\n",
    "my_layer.get_example = True\n",
    "my_layer.report_timing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 4096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    _, clm_batch = model.generate_token(clm_batch)\n",
    "my_layer.x_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer.report_timing = True\n",
    "my_layer.get_example = False\n",
    "\n",
    "linear_timings = []\n",
    "lora_timings = []\n",
    "for _ in range(100):\n",
    "    linear_time, lora_time = my_layer(my_layer.x_example)\n",
    "\n",
    "    linear_timings.append(linear_time)\n",
    "    lora_timings.append(lora_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004304062109440565\n",
      "0.6795510449446738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "linear_timings_np = np.array(linear_timings)\n",
    "lora_timings_np = np.array(lora_timings)\n",
    "\n",
    "print(linear_timings_np.sum())\n",
    "print(lora_timings_np.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
