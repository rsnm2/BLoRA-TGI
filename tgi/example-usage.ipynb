{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/rshaw/BLoRA-TGI/tgi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org\n",
      "\n",
      "\n",
      "\n",
      "Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer: \\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - \n",
      "\n",
      "\n",
      "\n",
      "Outline a five sentence short story about the Patriots winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience and determination proved too much for the Rams, as they secured their sixth Super Bowl victory.\n",
      "5. The\n",
      "\n",
      "\n",
      "\n",
      "### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response: \n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a wave of her hand\n",
      "\n",
      "\n",
      "\n",
      "### Instruction: Write a poem about the transformers Python library.\n",
      "### Response: \n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from threading import Thread\n",
    "import json\n",
    "\n",
    "r_tuples = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 125),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75)\n",
    "]\n",
    "\n",
    "url = \"http://localhost:5543/generate\"\n",
    "\n",
    "def request_task(r_tuple):\n",
    "    inputs, lora_id, max_new_tokens = r_tuple\n",
    "    obj = {\n",
    "        \"inputs\": inputs,\n",
    "        \"lora_id\": lora_id,\n",
    "        \"generate_parameters\": {\n",
    "            \"max_new_tokens\":max_new_tokens,\n",
    "        }\n",
    "    }\n",
    "    with requests.post(url, json=obj) as r:\n",
    "        dct = json.loads(r.text)\n",
    "        \n",
    "        print(f'{inputs} {dct[\"response_text\"]}')\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "request_ts = [\n",
    "    Thread(target=request_task, args=[r_tuple]) \n",
    "    for r_tuple in r_tuples\n",
    "]\n",
    "\n",
    "import time\n",
    "for request_t in request_ts:\n",
    "    request_t.start()\n",
    "    time.sleep(1.0)\n",
    "\n",
    "for request_t in request_ts:\n",
    "    request_t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextGenerationRouter - with batching task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbacf95c0ee46cc8e38a74ca2aecfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from router import TextGenerationRouter, batching_task\n",
    "from utils import GenerateRequest, GenerateParameters, GenerateRequestInputs\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "router = TextGenerationRouter(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 125),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75)\n",
    "]\n",
    "\n",
    "generate_request_inputs = [\n",
    "    GenerateRequestInputs(\n",
    "        inputs=inp[0],\n",
    "        lora_id=inp[1],\n",
    "        generate_parameters=GenerateParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ) for inp in inputs\n",
    "]\n",
    "\n",
    "generate_requests = [\n",
    "    GenerateRequest.from_gr_inputs(gr_inputs) \n",
    "    for gr_inputs in generate_request_inputs\n",
    "] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "b_thread = Thread(target=batching_task, args=[router])\n",
    "b_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting thread 0\n",
      "new request!\n",
      "starting thread 1\n",
      "starting thread 2\n",
      "starting thread 3\n",
      "starting thread 4\n",
      "starting thread 5\n",
      "Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer:The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org\n",
      "\n",
      "\n",
      "\n",
      "starting thread 6\n",
      "starting thread 7\n",
      "starting thread 8\n",
      "Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer:\\\n",
      "\n",
      "\n",
      "\n",
      "starting thread 9\n",
      "Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer:The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org\n",
      "\n",
      "\n",
      "\n",
      "Outline a five sentence short story about the Patriotswinning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3 the. Super In Bowl the. against Patri whoary equally Tom impress Brster were. determined4 make. history However once, again the. Patri\n",
      " experience With determ powerfulination off provedense too and forif theling R defenseams,, the as P theyats sixth the Super A BowlFC victory Championship. against\n",
      "\n",
      "\n",
      "\n",
      "### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      " affordAs until the she horizon discovered, mag youngical girl dream, life out. at wave ocean of. her She hand had\n",
      "\n",
      "\n",
      "\n",
      "### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data intoTransform newers, libraryAnd\n",
      " helpA analyze for and data visual processing.\n",
      "Transform can Python data library intoA, powerful\n",
      " toolAnd canize transform. data\n",
      " newers\n",
      ", helpA and for visual data\n",
      "\n",
      "\n",
      "\n",
      "Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer:\\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n < 2) return n;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't it the. sun That rose was over a a am namedu Lletily that stood could on bring the her beachs gaz toing With the a vast always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a wave of her hand\n",
      "\n",
      "\n",
      "\n",
      "### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      " forms Python\n",
      ", us powerful to toolize,\n",
      "Iters transform, new\n",
      " forms for help data us processing to, analyze\n",
      " andIt visual intoTransform forms Python, libraryAnd\n",
      " us vers toatile analyze toolize processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize\n",
      "\n",
      "\n",
      "\n",
      "Outline a five sentence short story about the Patriotswinning Super\n",
      " Bowl1,. they The faced New off England theots Los, Angeles led R byams their, legend had quarter anbackiveady ro,\n",
      " toots2'. and their much st secured domin theirated\n",
      " the5 Kansas. City The Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience and determination proved too much for the Rams, as they secured their sixth Super Bowl victory.\n",
      "5. The\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def submit_request_task(gr):\n",
    "    router.submit_request(gr)\n",
    "\n",
    "    tokens = []\n",
    "    generation = gr.response_stream.get()\n",
    "    while not generation.stopped:\n",
    "        tokens.append(generation.token_id)\n",
    "        generation = gr.response_stream.get()\n",
    "\n",
    "    full_str = gr.inputs + router.service.model.tokenizer.decode(tokens)\n",
    "    print(full_str)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "r_threads = [\n",
    "    Thread(target=submit_request_task, args=[gr])\n",
    "    for gr in generate_requests\n",
    "]\n",
    "\n",
    "for idx, r_thread in enumerate(r_threads):\n",
    "    print(f\"starting thread {idx}\")\n",
    "    r_thread.start()\n",
    "    time.sleep(1.0)\n",
    "\n",
    "for r_thread in r_threads:\n",
    "    r_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "router.stop_batching_task()\n",
    "b_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextGenerationRouter - without batching task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306c4dd532f74f9e8fd54dba62a6f839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from router import TextGenerationRouter, batching_task\n",
    "from utils import GenerateRequest, GenerateParameters, GenerateRequestInputs\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "router = TextGenerationRouter(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 125),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75)\n",
    "]\n",
    "\n",
    "generate_request_inputs = [\n",
    "    GenerateRequestInputs(\n",
    "        inputs=inp[0],\n",
    "        lora_id=inp[1],\n",
    "        generate_parameters=GenerateParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ) for inp in inputs\n",
    "]\n",
    "\n",
    "gr_lst = [\n",
    "    GenerateRequest.from_gr_inputs(gr_inputs) \n",
    "    for gr_inputs in generate_request_inputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerateRequest(inputs='Outline a five sentence short story about the Patriots', lora_id='jondurbin/airoboros-7b-gpt4-1.2-peft', generate_parameters=GenerateParameters(max_new_tokens=125), response_stream=<queue.Queue object at 0x7fd7011ee430>)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "# first prefill\n",
    "print(gr_lst[idx])\n",
    "router.submit_request(gr_lst[idx])\n",
    "idx += 1\n",
    "\n",
    "next_batch = router.queue.next_batch(block=False)\n",
    "assert next_batch is not None\n",
    "batch, generate_requests = next_batch\n",
    "\n",
    "cached_batch = router.prefill(\n",
    "    batch=batch,\n",
    "    generate_requests=generate_requests\n",
    ")\n",
    "\n",
    "# run a few decodes\n",
    "next_batch = router.queue.next_batch(block=False)\n",
    "assert next_batch is None\n",
    "\n",
    "for _ in range(10):\n",
    "    if cached_batch is None:\n",
    "        break\n",
    "    \n",
    "    batches = [cached_batch]\n",
    "    cached_batch = router.decode(\n",
    "        batches=batches,\n",
    "        generate_requests=generate_requests\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerateRequest(inputs='Question: What are the various algorithms to sort a list?\\n\\nAnswer:', lora_id='trl-lib/llama-7b-se-rl-peft', generate_parameters=GenerateParameters(max_new_tokens=50), response_stream=<queue.Queue object at 0x7fd7011ee880>)\n",
      "GenerateRequest(inputs='### Instruction: Write a poem about the transformers Python library.\\n### Response:', lora_id='winddude/wizardLM-LlaMA-LoRA-7B', generate_parameters=GenerateParameters(max_new_tokens=100), response_stream=<queue.Queue object at 0x7fd7011eea60>)\n",
      "GenerateRequest(inputs='Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:', lora_id='trl-lib/llama-7b-se-rl-peft', generate_parameters=GenerateParameters(max_new_tokens=50), response_stream=<queue.Queue object at 0x7fd7011eebb0>)\n",
      "GenerateRequest(inputs='### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:', lora_id='winddude/wizardLM-LlaMA-LoRA-7B', generate_parameters=GenerateParameters(max_new_tokens=75), response_stream=<queue.Queue object at 0x7fd7011eecd0>)\n"
     ]
    }
   ],
   "source": [
    "NUM_DECODES = 35\n",
    "\n",
    "while idx < len(gr_lst):\n",
    "    batches = [cached_batch]\n",
    "    \n",
    "    # add a prefill\n",
    "    print(gr_lst[idx])\n",
    "    router.submit_request(gr_lst[idx])\n",
    "    idx += 1\n",
    "\n",
    "    next_batch = router.queue.next_batch(block=False)\n",
    "    assert next_batch is not None\n",
    "    new_batch, new_generate_requests = next_batch\n",
    "\n",
    "    new_cached_batch = router.prefill(\n",
    "        batch=new_batch,\n",
    "        generate_requests=new_generate_requests\n",
    "    )\n",
    "\n",
    "    if new_cached_batch is not None:\n",
    "        batches.append(new_cached_batch)\n",
    "        assert len(generate_requests.keys() & new_generate_requests.keys()) == 0\n",
    "        generate_requests.update(new_generate_requests)\n",
    "\n",
    "    # decode\n",
    "    cached_batch = router.decode(\n",
    "        batches=batches,\n",
    "        generate_requests=generate_requests\n",
    "    )\n",
    "\n",
    "    # run decodes\n",
    "    for i in range(NUM_DECODES):\n",
    "        if cached_batch is None:\n",
    "            break\n",
    "\n",
    "        batches = [cached_batch]\n",
    "        cached_batch = router.decode(\n",
    "            batches=batches,\n",
    "            generate_requests=generate_requests\n",
    "        )\n",
    "\n",
    "while cached_batch is not None:\n",
    "    batches = [cached_batch]\n",
    "    cached_batch = router.decode(\n",
    "        batches=batches,\n",
    "        generate_requests=generate_requests\n",
    "    )\n",
    "    batches = [cached_batch]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience and determination proved too much for the Rams, as they secured their sixth Super Bowl victory.\n",
      "5. The\n",
      "\n",
      "\n",
      "\n",
      "The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize\n",
      "\n",
      "\n",
      "\n",
      "\\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n < 2) return n;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a wave of her hand\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gr in gr_lst:\n",
    "    tokens = []\n",
    "\n",
    "    generation = gr.response_stream.get()\n",
    "    while not generation.stopped:\n",
    "        tokens.append(generation.token_id)\n",
    "        generation = gr.response_stream.get()\n",
    "\n",
    "    print(router.service.model.tokenizer.decode(tokens))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextGenerationService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb915202b2374eb5be54807c3c246347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from service.service import TextGenerationService\n",
    "from utils import Batch, Request, GenerationParameters\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "service = TextGenerationService(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 125),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75)\n",
    "]\n",
    "\n",
    "requests = []\n",
    "\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generation_parameters=GenerationParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ))\n",
    "\n",
    "batches = []\n",
    "for idx, request in enumerate(requests):\n",
    "    batches.append(Batch(id=idx, requests=[request]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = []\n",
    "generation_dict = {}\n",
    "\n",
    "for batch in batches:\n",
    "    for req in batch.requests:\n",
    "        generation_dict[req.id] = []\n",
    "        input_seqs.append(service.model.tokenizer(req.inputs).input_ids)\n",
    "\n",
    "active_ids = set()\n",
    "tokens_b4_prefill = [25, 15, 10, 25, 100]\n",
    "\n",
    "iterator = zip(batches, tokens_b4_prefill)\n",
    "\n",
    "cached_batches = []\n",
    "for idx, (batch, tokens_to_generate) in enumerate(iterator):\n",
    "    generations, cached_b = service.Prefill(batch)\n",
    "    for gen in generations:\n",
    "        generation_dict[gen.request_id].append(gen.token_id)\n",
    "    \n",
    "    cached_batches.append(cached_b)\n",
    "\n",
    "    should_break = False\n",
    "    for _ in range(tokens_to_generate):\n",
    "        generations, cached_b = service.Decode(cached_batches)\n",
    "\n",
    "        for gen in generations:\n",
    "            generation_dict[gen.request_id].append(gen.token_id)\n",
    "            if gen.stopped:\n",
    "                if cached_b is None:\n",
    "                    should_break = True\n",
    "                    break\n",
    "                request_ids = cached_b.request_ids.copy()\n",
    "                request_ids.remove(gen.request_id)\n",
    "                cached_b = service.FilterBatch(cached_b.batch_id, request_ids=request_ids)\n",
    "\n",
    "        if should_break:\n",
    "            break\n",
    "        \n",
    "        cached_batches = [cached_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Outline a five sentence short story about the Patriots winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience and determination proved too much for the Rams, as they secured their sixth Super Bowl victory.\n",
      "5. The Patri\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org/\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer: \\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - 1\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a wave of her hand,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_gen(tokenizer, input_seqs, generation_dict):\n",
    "    for idx in generation_dict:\n",
    "        tokens = input_seqs[idx].copy()    \n",
    "        tokens.extend(generation_dict[idx])\n",
    "    \n",
    "        print(tokenizer.decode(tokens))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "print_gen(service.model.tokenizer, input_seqs, generation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLoraCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from service.causal_lm import BLoraCausalLMBatch, BLoraCausalLM\n",
    "from utils import Batch, Request, GenerationParameters\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "model = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")\n",
    "\n",
    "def prefill(model, batch, gen_dict):\n",
    "    generations, batch = model.generate_token(batch)\n",
    "    for gen in generations:\n",
    "        gen_dict[gen.request_id].append(gen.token_id)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def decode(model, batch, gen_dict, active_ids):\n",
    "    stopped_ids = []\n",
    "    generations, batch = model.generate_token(batch)\n",
    "\n",
    "    for gen in generations:\n",
    "        if gen.stopped:\n",
    "            stopped_ids.append(gen.request_id)    \n",
    "        gen_dict[gen.request_id].append(gen.token_id)\n",
    "\n",
    "    if len(stopped_ids) > 0:\n",
    "        if batch is None:\n",
    "            return batch\n",
    "        for stopped_id in stopped_ids:\n",
    "            active_ids.remove(stopped_id)\n",
    "        batch.filter(list(active_ids))\n",
    "\n",
    "    return batch\n",
    "\n",
    "def print_gen(tokenizer, input_seqs, generation_dict):\n",
    "    for idx in generation_dict:\n",
    "        tokens = input_seqs[idx].copy()    \n",
    "        tokens.extend(generation_dict[idx])\n",
    "    \n",
    "        print(tokenizer.decode(tokens))\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 100),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 25),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 50)\n",
    "]\n",
    "\n",
    "requests = []\n",
    "\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generation_parameters=GenerationParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ))\n",
    "\n",
    "batches = []\n",
    "for idx, request in enumerate(requests):\n",
    "    batches.append(Batch(id=idx, requests=[request]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Outline a five sentence short story about the Patriots winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer: \\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n < 2) return n;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clm_batches = [BLoraCausalLMBatch.from_batch(batch, tokenizer=model.tokenizer, device=\"cuda\") for batch in batches]\n",
    "input_seqs = []\n",
    "generation_dict = {}\n",
    "for clm_batch in clm_batches:\n",
    "    input_seqs.extend(clm_batch.input_ids.tolist())\n",
    "    for req in clm_batch.requests:\n",
    "        generation_dict[req.id] = []\n",
    "\n",
    "active_ids = set()\n",
    "tokens_b4_prefill = [25, 15, 10, 25, 100]\n",
    "\n",
    "# PREFILL\n",
    "clm_batch = prefill(model, clm_batches[0], generation_dict)\n",
    "for r in clm_batch.requests:\n",
    "    active_ids.add(r.id)\n",
    "\n",
    "# DECODE\n",
    "for _ in range(tokens_b4_prefill[0]):\n",
    "    clm_batch = decode(model, clm_batch, generation_dict, active_ids)\n",
    "\n",
    "iterator = zip(clm_batches[1:], tokens_b4_prefill[1:])\n",
    "for clm_batch_new, tokens_to_generate in iterator:\n",
    "\n",
    "    # PREFILL\n",
    "    clm_batch_new = prefill(model, clm_batch_new, generation_dict)\n",
    "    for r in clm_batch_new.requests:\n",
    "        active_ids.add(r.id)\n",
    "\n",
    "    # CONCATENATE\n",
    "    clm_batch = BLoraCausalLMBatch.concatenate(\n",
    "        batches=[clm_batch, clm_batch_new]\n",
    "    )\n",
    "\n",
    "    # DECODE LOOP\n",
    "    for i in range(tokens_to_generate):\n",
    "        clm_batch = decode(model, clm_batch, generation_dict, active_ids)\n",
    "\n",
    "        if clm_batch is None:\n",
    "            break\n",
    "\n",
    "print_gen(model.tokenizer, input_seqs, generation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>Outline a five sentence short story about the Patriots winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience\n",
      "\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia\n",
      "\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk>Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer: \\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n < 2) return n;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = Batch(id=0, requests=requests)\n",
    "clm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model.tokenizer, device=\"cuda\")\n",
    "\n",
    "input_seqs = clm_batch.input_ids.tolist()\n",
    "generation_dict = {req.id : [] for req in clm_batch.requests}\n",
    "\n",
    "active_ids = set()\n",
    "\n",
    "# PREFILL\n",
    "clm_batch = prefill(model, clm_batch, generation_dict)\n",
    "for r in clm_batch.requests:\n",
    "    active_ids.add(r.id)\n",
    "\n",
    "# DECODE\n",
    "while clm_batch is not None:\n",
    "    clm_batch = decode(model, clm_batch, generation_dict, active_ids)\n",
    "\n",
    "print_gen(model.tokenizer, input_seqs, generation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from service.blora_utils import load_loras, prepare_batch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model2 = LlamaForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = 0\n",
    "model2, lora_map = load_loras(model2, lora_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jondurbin/airoboros-7b-gpt4-1.2-peft:\n",
      "Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.\n",
      "The character, who is a young boy named Timmy, stumbles upon a secret room in his house that contained relics from his future. The room was hidden behind a bookcase in the library, and it was filled with strange artifacts and documents.\n",
      "Timmy's curiosity got the best of him, and he decided\n",
      "\n",
      "trl-lib/llama-7b-se-rl-peft:\n",
      "Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer:: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org/wiki/Selection_sort)\n",
      "\\item [Insertion sort](http://en.wikipedia\n",
      "\n",
      "winddude/wizardLM-LlaMA-LoRA-7B:\n",
      "### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers\n",
      "\n",
      "trl-lib/llama-7b-se-rl-peft:\n",
      "Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer:0\n",
      "\n",
      "\\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end{code}\n",
      "\n",
      "winddude/wizardLM-LlaMA-LoRA-7B:\n",
      "### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a\n"
     ]
    }
   ],
   "source": [
    "b = prepare_batch(inputs, tokenizer, model2, lora_map)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for out in model2.generate(\n",
    "    **b,\n",
    "    max_length=100,\n",
    "    stream_output=True\n",
    "):\n",
    "    outputs.append(out)\n",
    "    batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))\n",
    "    clear_output(wait=True)\n",
    "    print(\"\\n\\n\".join([lora + \":\\n\" + prompt + decoded for (prompt, lora, _), decoded in zip(inputs, batch_decoded)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unset_batch_lora_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m batch \u001b[39m=\u001b[39m Batch(idx\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, requests\u001b[39m=\u001b[39;49mrequests)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m clm_batch \u001b[39m=\u001b[39m BLoraCausalLMBatch\u001b[39m.\u001b[39mfrom_batch(batch, tokenizer\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mtokenizer, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m input_seqs \u001b[39m=\u001b[39m clm_batch\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'idx'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = prepare_batch(inputs, tokenizer, model, lora_map)\n",
    "\n",
    "# tokens = b.input_ids[0,:].tolist()\n",
    "\n",
    "# model_kwargs = {\n",
    "#     \"attention_mask\": b.attention_mask,\n",
    "#     \"use_cache\": True\n",
    "# }\n",
    "\n",
    "# input_ids = b.input_ids\n",
    "\n",
    "# for _ in range(100):\n",
    "#     model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "#     outputs = model(\n",
    "#         **model_inputs,\n",
    "#         return_dict=True,\n",
    "#         output_attentions=False,\n",
    "#         output_hidden_states=False,\n",
    "#     )\n",
    "\n",
    "#     next_token_logits = outputs.logits[:, -1, :]\n",
    "#     next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "#     tokens.append(next_tokens.item())\n",
    "\n",
    "#     input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "#     model_kwargs = model._update_model_kwargs_for_generation(\n",
    "#         outputs, model_kwargs, is_encoder_decoder=False\n",
    "#     )\n",
    "\n",
    "#     print(next_tokens.item())\n",
    "#     print(\"\\n\\n\")\n",
    "#     # print(len(tokens))\n",
    "#     # print(tokenizer.decode(tokens[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 33/33 [00:08<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "Loading LORAs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id=0, requests=[Request(id=0, lora_id='trl-lib/llama-7b-se-rl-peft', inputs='Write a 6 line dialogue between a character and a magical creature that only they can see.')])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0]\n",
    "    ))\n",
    "\n",
    "batch = Batch(\n",
    "    id=0,\n",
    "    requests=requests\n",
    ")\n",
    "\n",
    "causal_lm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model2.tokenizer, device=\"cuda\")\n",
    "generation_dct = {idx: [] for idx in range(len(causal_lm_batch.requests))}\n",
    "\n",
    "for _ in range(100):\n",
    "    # print(causal_lm_batch.input_ids)\n",
    "    # print(causal_lm_batch.position_ids)\n",
    "    # # print(causal_lm_batch.attention_mask)\n",
    "    # if causal_lm_batch.past_key_values is None:\n",
    "    #     print(\"None\")\n",
    "    # else:\n",
    "    #     print(causal_lm_batch.past_key_values[0][0][0,0,0,0])\n",
    "    #     print(causal_lm_batch.past_key_values[0][0][0,0,0,1])\n",
    "\n",
    "    generations, causal_lm_batch = model2.generate_token(causal_lm_batch)\n",
    "    for idx, gen in enumerate(generations):\n",
    "        generation_dct[idx].append(gen.token_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical\n"
     ]
    }
   ],
   "source": [
    "causal_lm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model2.tokenizer, device=\"cuda\")\n",
    "input_seqs = causal_lm_batch.input_ids.tolist()\n",
    "\n",
    "key = 0\n",
    "input_seqs[key].extend(generation_dct[key])\n",
    "print(model2.tokenizer.decode(input_seqs[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.active_batch_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jondurbin/airoboros-7b-gpt4-1.2-peft']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.model.batch_lora_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in generation_dct:\n",
    "    print(generation_dct[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
