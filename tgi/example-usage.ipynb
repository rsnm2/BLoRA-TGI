{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/rshaw/BLoRA-TGI/tgi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513f296e59594ae2ba8fd6f7bac510fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from service.causal_lm import BLoraCausalLMBatch, BLoraCausalLM\n",
    "from utils import Batch, Request, GenerationParameters\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "model = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 50),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 100),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 25),\n",
    "    ('Question: Sculpt a three verse poem about the feeling of walking through a lush, vibrant garden in full bloom.\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 125),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 50)\n",
    "]\n",
    "\n",
    "requests = []\n",
    "new_tokens = []\n",
    "\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generation_parameters=GenerationParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ))\n",
    "\n",
    "batch = Batch(\n",
    "    id=0,\n",
    "    requests=requests\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.\n",
      "The character, who is a young boy named Timmy, stumbles upon a secret room in his house that contained relics from his future. The room was hidden behind a bookcase in the library, and it was filled with strange\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk>Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk>Sculpt a three verse poem about the feeling of walking through a lush, vibrant garden in full bloom.\n",
      "The poem should be written in the first person.\n",
      "The poem should be written in free verse.\n",
      "The poem should be written in a form of poetry that is not a sonnet.\n",
      "The poem should be written in a form of poetry that is not a haiku.\n",
      "The poem should be written in a form of poetry that is not a limerick.\n",
      "The poem should be written in a form of poetry that is not a cinquain.\n",
      "The poem should be written in a form of poetry that is not a diamante.\n",
      "The poem should be written in a form of poetry that\n",
      "\n",
      "\n",
      "<unk><unk>### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal_lm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model.tokenizer, device=\"cuda\")\n",
    "input_seqs = causal_lm_batch.input_ids.tolist()\n",
    "\n",
    "generation_dct = {r.id: [] for r in causal_lm_batch.requests}\n",
    "active_ids = set([r.id for r in requests])\n",
    "\n",
    "while True:\n",
    "    stopped_ids = []\n",
    "\n",
    "    generations, causal_lm_batch = model.generate_token(causal_lm_batch)\n",
    "    for idx, gen in enumerate(generations):\n",
    "        if gen.stopped:\n",
    "            stopped_ids.append(gen.request_id)    \n",
    "        generation_dct[gen.request_id].append(gen.token_id)\n",
    "\n",
    "    if len(stopped_ids) > 0:\n",
    "        if causal_lm_batch is None:\n",
    "            break\n",
    "\n",
    "        for stopped_id in stopped_ids:\n",
    "            active_ids.remove(stopped_id)\n",
    "        causal_lm_batch.filter(list(active_ids))\n",
    "\n",
    "for key in generation_dct:\n",
    "    input_seqs[key].extend(generation_dct[key])\n",
    "    print(model.tokenizer.decode(input_seqs[key]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from service.blora_utils import load_loras, prepare_batch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model2 = LlamaForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = 0\n",
    "model2, lora_map = load_loras(model2, lora_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jondurbin/airoboros-7b-gpt4-1.2-peft:\n",
      "Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.\n",
      "The character, who is a young boy named Timmy, stumbles upon a secret room in his house that contained relics from his future. The room was hidden behind a bookcase in the library, and it was filled with strange artifacts and documents.\n",
      "Timmy's curiosity got the best of him, and he decided\n",
      "\n",
      "trl-lib/llama-7b-se-rl-peft:\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. The creature must be a magical creature that is not a human.\n",
      "Write a 6 line dialogue between a character and\n",
      "\n",
      "winddude/wizardLM-LlaMA-LoRA-7B:\n",
      "Describe a four sentence scene where a character discovers a hidden talent that changes their life forever.\n",
      "What is the name of the character?\n",
      "What is the talent?\n",
      "What is the setting of the scene?\n",
      "What is the character’s reaction to the discovery?\n",
      "What is the impact of the talent on the character’s life?\n",
      "What is the character’s next step?\n",
      "What is the name of the character?\n",
      "\n",
      "trl-lib/llama-7b-se-rl-peft:\n",
      "Sculpt a three verse poem about the feeling of walking through a lush, vibrant garden in full bloom.\n",
      "The poem should be written in the first person.\n",
      "The poem should be written in free verse.\n",
      "The poem should be written in a form of poetry that is not a sonnet.\n",
      "The poem should be written in a form of poetry that is not a haiku.\n",
      "The poem should be written in a form of poetry that is\n",
      "\n",
      "winddude/wizardLM-LlaMA-LoRA-7B:\n",
      "Develop an eight sentence short story about a character who can bring their dreams into reality, but only for a limited time.\n",
      "10. 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "b = prepare_batch(inputs, tokenizer, model2, lora_map)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for out in model2.generate(\n",
    "    **b,\n",
    "    max_length=100,\n",
    "    stream_output=True\n",
    "):\n",
    "    outputs.append(out)\n",
    "    batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))\n",
    "    clear_output(wait=True)\n",
    "    print(\"\\n\\n\".join([lora + \":\\n\" + prompt + decoded for (prompt, lora, _), decoded in zip(inputs, batch_decoded)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rshaw/BLoRA-TGI/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.79it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = prepare_batch(inputs, tokenizer, model, lora_map)\n",
    "\n",
    "# tokens = b.input_ids[0,:].tolist()\n",
    "\n",
    "# model_kwargs = {\n",
    "#     \"attention_mask\": b.attention_mask,\n",
    "#     \"use_cache\": True\n",
    "# }\n",
    "\n",
    "# input_ids = b.input_ids\n",
    "\n",
    "# for _ in range(100):\n",
    "#     model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "#     outputs = model(\n",
    "#         **model_inputs,\n",
    "#         return_dict=True,\n",
    "#         output_attentions=False,\n",
    "#         output_hidden_states=False,\n",
    "#     )\n",
    "\n",
    "#     next_token_logits = outputs.logits[:, -1, :]\n",
    "#     next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "#     tokens.append(next_tokens.item())\n",
    "\n",
    "#     input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "#     model_kwargs = model._update_model_kwargs_for_generation(\n",
    "#         outputs, model_kwargs, is_encoder_decoder=False\n",
    "#     )\n",
    "\n",
    "#     print(next_tokens.item())\n",
    "#     print(\"\\n\\n\")\n",
    "#     # print(len(tokens))\n",
    "#     # print(tokenizer.decode(tokens[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "Loading LORAs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id=0, requests=[Request(id=0, lora_id='trl-lib/llama-7b-se-rl-peft', inputs='Write a 6 line dialogue between a character and a magical creature that only they can see.')])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0]\n",
    "    ))\n",
    "\n",
    "batch = Batch(\n",
    "    id=0,\n",
    "    requests=requests\n",
    ")\n",
    "\n",
    "causal_lm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model2.tokenizer, device=\"cuda\")\n",
    "generation_dct = {idx: [] for idx in range(len(causal_lm_batch.requests))}\n",
    "\n",
    "for _ in range(100):\n",
    "    # print(causal_lm_batch.input_ids)\n",
    "    # print(causal_lm_batch.position_ids)\n",
    "    # # print(causal_lm_batch.attention_mask)\n",
    "    # if causal_lm_batch.past_key_values is None:\n",
    "    #     print(\"None\")\n",
    "    # else:\n",
    "    #     print(causal_lm_batch.past_key_values[0][0][0,0,0,0])\n",
    "    #     print(causal_lm_batch.past_key_values[0][0][0,0,0,1])\n",
    "\n",
    "    generations, causal_lm_batch = model2.generate_token(causal_lm_batch)\n",
    "    for idx, gen in enumerate(generations):\n",
    "        generation_dct[idx].append(gen.token_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical\n"
     ]
    }
   ],
   "source": [
    "causal_lm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model2.tokenizer, device=\"cuda\")\n",
    "input_seqs = causal_lm_batch.input_ids.tolist()\n",
    "\n",
    "key = 0\n",
    "input_seqs[key].extend(generation_dct[key])\n",
    "print(model2.tokenizer.decode(input_seqs[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.active_batch_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jondurbin/airoboros-7b-gpt4-1.2-peft']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.model.batch_lora_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in generation_dct:\n",
    "    print(generation_dct[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
