{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/rshaw/BLoRA-TGI/tgi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextGenerationRouter - without batching task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535843fe21a946a1a53519703811fcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from router import TextGenerationRouter, batching_task\n",
    "from utils import GenerateRequest, GenerateParameters, GenerateRequestInputs\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "router = TextGenerationRouter(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 125),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75)\n",
    "]\n",
    "\n",
    "generate_request_inputs = [\n",
    "    GenerateRequestInputs(\n",
    "        inputs=inp[0],\n",
    "        lora_id=inp[1],\n",
    "        generate_parameters=GenerateParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ) for inp in inputs\n",
    "]\n",
    "\n",
    "gr_lst = [\n",
    "    GenerateRequest.from_gr_inputs(gr_inputs) \n",
    "    for gr_inputs in generate_request_inputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerateRequest(inputs='Outline a five sentence short story about the Patriots', lora_id='jondurbin/airoboros-7b-gpt4-1.2-peft', generate_parameters=GenerateParameters(max_new_tokens=125), response_stream=<queue.Queue object at 0x7f1dc8f718e0>)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "# first prefill\n",
    "print(gr_lst[idx])\n",
    "router.submit_request(gr_lst[idx])\n",
    "idx += 1\n",
    "\n",
    "next_batch = router.queue.next_batch(block=False)\n",
    "assert next_batch is not None\n",
    "batch, generate_requests = next_batch\n",
    "\n",
    "cached_batch = router.prefill(\n",
    "    batch=batch,\n",
    "    generate_requests=generate_requests\n",
    ")\n",
    "\n",
    "# run a few decodes\n",
    "next_batch = router.queue.next_batch(block=False)\n",
    "assert next_batch is None\n",
    "\n",
    "for _ in range(10):\n",
    "    if cached_batch is None:\n",
    "        break\n",
    "    \n",
    "    batches = [cached_batch]\n",
    "    cached_batch = router.decode(\n",
    "        batches=batches,\n",
    "        generate_requests=generate_requests\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerateRequest(inputs='Question: What are the various algorithms to sort a list?\\n\\nAnswer:', lora_id='trl-lib/llama-7b-se-rl-peft', generate_parameters=GenerateParameters(max_new_tokens=50), response_stream=<queue.Queue object at 0x7f1dc8f73910>)\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Batch ID 1 not found in cache.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m cached_batch \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m cached_batch \u001b[39m=\u001b[39m router\u001b[39m.\u001b[39;49mdecode(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     batches\u001b[39m=\u001b[39;49mbatches,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     generate_requests\u001b[39m=\u001b[39;49mgenerate_requests\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m batches \u001b[39m=\u001b[39m [cached_batch]    \n",
      "File \u001b[0;32m~/rshaw/BLoRA-TGI/tgi/router.py:58\u001b[0m, in \u001b[0;36mTextGenerationRouter.decode\u001b[0;34m(self, batches, generate_requests)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     54\u001b[0m     batches: List[CachedBatch],\n\u001b[1;32m     55\u001b[0m     generate_requests: Dict[\u001b[39mint\u001b[39m,GenerateRequest]\n\u001b[1;32m     56\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[CachedBatch]:\n\u001b[0;32m---> 58\u001b[0m     generations, next_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mDecode(batches\u001b[39m=\u001b[39;49mbatches)\n\u001b[1;32m     59\u001b[0m     active_generate_request_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_send_generations(generations, generate_requests)\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_batch(batch\u001b[39m=\u001b[39mnext_batch, active_generate_request_ids\u001b[39m=\u001b[39mactive_generate_request_ids)\n",
      "File \u001b[0;32m~/rshaw/BLoRA-TGI/tgi/service/service.py:73\u001b[0m, in \u001b[0;36mTextGenerationService.Decode\u001b[0;34m(self, batches)\u001b[0m\n\u001b[1;32m     71\u001b[0m clm_batches \u001b[39m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m cached_batch \u001b[39min\u001b[39;00m batches:\n\u001b[0;32m---> 73\u001b[0m     clm_batches\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache\u001b[39m.\u001b[39;49mpop(cached_batch\u001b[39m.\u001b[39;49mbatch_id))\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(clm_batches) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     76\u001b[0m     clm_batch \u001b[39m=\u001b[39m BLoraCausalLMBatch\u001b[39m.\u001b[39mconcatenate(clm_batches)\n",
      "File \u001b[0;32m~/rshaw/BLoRA-TGI/tgi/service/service.py:13\u001b[0m, in \u001b[0;36mBatchCache.pop\u001b[0;34m(self, batch_id)\u001b[0m\n\u001b[1;32m     11\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mpop(batch_id, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch ID \u001b[39m\u001b[39m{\u001b[39;00mbatch_id\u001b[39m}\u001b[39;00m\u001b[39m not found in cache.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mValueError\u001b[0m: Batch ID 1 not found in cache."
     ]
    }
   ],
   "source": [
    "# add a prefill\n",
    "print(gr_lst[idx])\n",
    "router.submit_request(gr_lst[idx])\n",
    "idx += 1\n",
    "\n",
    "next_batch = router.queue.next_batch(block=False)\n",
    "assert next_batch is not None\n",
    "new_batch, new_generate_requests = next_batch\n",
    "\n",
    "new_cached_batch = router.prefill(\n",
    "    batch=new_batch,\n",
    "    generate_requests=new_generate_requests\n",
    ")\n",
    "\n",
    "if new_cached_batch is not None:\n",
    "    batches.append(new_cached_batch)\n",
    "    assert len(generate_requests.keys() & new_generate_requests.keys()) == 0\n",
    "    generate_requests.update(new_generate_requests)\n",
    "\n",
    "# decode\n",
    "cached_batch = router.decode(\n",
    "    batches=batches,\n",
    "    generate_requests=generate_requests\n",
    ")\n",
    "\n",
    "# run decodes\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    if cached_batch is None:\n",
    "        break\n",
    "\n",
    "    batches = [cached_batch]\n",
    "    cached_batch = router.decode(\n",
    "        batches=batches,\n",
    "        generate_requests=generate_requests\n",
    "    )\n",
    "    batches = [cached_batch]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CachedBatch(batch_id=0, request_ids=[0]),\n",
       " CachedBatch(batch_id=1, request_ids=[1])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router.service.cache.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextGenerationService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb915202b2374eb5be54807c3c246347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from service.service import TextGenerationService\n",
    "from utils import Batch, Request, GenerationParameters\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "service = TextGenerationService(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 125),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 100),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75)\n",
    "]\n",
    "\n",
    "requests = []\n",
    "\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generation_parameters=GenerationParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ))\n",
    "\n",
    "batches = []\n",
    "for idx, request in enumerate(requests):\n",
    "    batches.append(Batch(id=idx, requests=[request]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = []\n",
    "generation_dict = {}\n",
    "\n",
    "for batch in batches:\n",
    "    for req in batch.requests:\n",
    "        generation_dict[req.id] = []\n",
    "        input_seqs.append(service.model.tokenizer(req.inputs).input_ids)\n",
    "\n",
    "active_ids = set()\n",
    "tokens_b4_prefill = [25, 15, 10, 25, 100]\n",
    "\n",
    "iterator = zip(batches, tokens_b4_prefill)\n",
    "\n",
    "cached_batches = []\n",
    "for idx, (batch, tokens_to_generate) in enumerate(iterator):\n",
    "    generations, cached_b = service.Prefill(batch)\n",
    "    for gen in generations:\n",
    "        generation_dict[gen.request_id].append(gen.token_id)\n",
    "    \n",
    "    cached_batches.append(cached_b)\n",
    "\n",
    "    should_break = False\n",
    "    for _ in range(tokens_to_generate):\n",
    "        generations, cached_b = service.Decode(cached_batches)\n",
    "\n",
    "        for gen in generations:\n",
    "            generation_dict[gen.request_id].append(gen.token_id)\n",
    "            if gen.stopped:\n",
    "                if cached_b is None:\n",
    "                    should_break = True\n",
    "                    break\n",
    "                request_ids = cached_b.request_ids.copy()\n",
    "                request_ids.remove(gen.request_id)\n",
    "                cached_b = service.FilterBatch(cached_b.batch_id, request_ids=request_ids)\n",
    "\n",
    "        if should_break:\n",
    "            break\n",
    "        \n",
    "        cached_batches = [cached_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Outline a five sentence short story about the Patriots winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience and determination proved too much for the Rams, as they secured their sixth Super Bowl victory.\n",
      "5. The Patri\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org/\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer: \\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - 1\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a wave of her hand,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_gen(tokenizer, input_seqs, generation_dict):\n",
    "    for idx in generation_dict:\n",
    "        tokens = input_seqs[idx].copy()    \n",
    "        tokens.extend(generation_dict[idx])\n",
    "    \n",
    "        print(tokenizer.decode(tokens))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "print_gen(service.model.tokenizer, input_seqs, generation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLoraCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from service.causal_lm import BLoraCausalLMBatch, BLoraCausalLM\n",
    "from utils import Batch, Request, GenerationParameters\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "\n",
    "base_model_id = \"decapoda-research/llama-7b-hf\"\n",
    "lora_ids = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", 'winddude/wizardLM-LlaMA-LoRA-7B']\n",
    "\n",
    "model = BLoraCausalLM(\n",
    "    base_model_id=base_model_id,\n",
    "    lora_ids=lora_ids\n",
    ")\n",
    "\n",
    "def prefill(model, batch, gen_dict):\n",
    "    generations, batch = model.generate_token(batch)\n",
    "    for gen in generations:\n",
    "        gen_dict[gen.request_id].append(gen.token_id)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def decode(model, batch, gen_dict, active_ids):\n",
    "    stopped_ids = []\n",
    "    generations, batch = model.generate_token(batch)\n",
    "\n",
    "    for gen in generations:\n",
    "        if gen.stopped:\n",
    "            stopped_ids.append(gen.request_id)    \n",
    "        gen_dict[gen.request_id].append(gen.token_id)\n",
    "\n",
    "    if len(stopped_ids) > 0:\n",
    "        if batch is None:\n",
    "            return batch\n",
    "        for stopped_id in stopped_ids:\n",
    "            active_ids.remove(stopped_id)\n",
    "        batch.filter(list(active_ids))\n",
    "\n",
    "    return batch\n",
    "\n",
    "def print_gen(tokenizer, input_seqs, generation_dict):\n",
    "    for idx in generation_dict:\n",
    "        tokens = input_seqs[idx].copy()    \n",
    "        tokens.extend(generation_dict[idx])\n",
    "    \n",
    "        print(tokenizer.decode(tokens))\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ('Outline a five sentence short story about the Patriots',\n",
    "    'jondurbin/airoboros-7b-gpt4-1.2-peft', 100),\n",
    "    ('Question: What are the various algorithms to sort a list?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 25),\n",
    "    ('### Instruction: Write a poem about the transformers Python library.\\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 75),\n",
    "    ('Question: How can I write a Java function to generate the nth Fibonacci number?\\n\\nAnswer:',\n",
    "    'trl-lib/llama-7b-se-rl-peft', 50),\n",
    "    ('### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \\n### Response:',\n",
    "    'winddude/wizardLM-LlaMA-LoRA-7B', 50)\n",
    "]\n",
    "\n",
    "requests = []\n",
    "\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0],\n",
    "        generation_parameters=GenerationParameters(\n",
    "            max_new_tokens=inp[2]\n",
    "        )\n",
    "    ))\n",
    "\n",
    "batches = []\n",
    "for idx, request in enumerate(requests):\n",
    "    batches.append(Batch(id=idx, requests=[request]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Outline a five sentence short story about the Patriots winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "<unk>Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer: \\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n < 2) return n;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clm_batches = [BLoraCausalLMBatch.from_batch(batch, tokenizer=model.tokenizer, device=\"cuda\") for batch in batches]\n",
    "input_seqs = []\n",
    "generation_dict = {}\n",
    "for clm_batch in clm_batches:\n",
    "    input_seqs.extend(clm_batch.input_ids.tolist())\n",
    "    for req in clm_batch.requests:\n",
    "        generation_dict[req.id] = []\n",
    "\n",
    "active_ids = set()\n",
    "tokens_b4_prefill = [25, 15, 10, 25, 100]\n",
    "\n",
    "# PREFILL\n",
    "clm_batch = prefill(model, clm_batches[0], generation_dict)\n",
    "for r in clm_batch.requests:\n",
    "    active_ids.add(r.id)\n",
    "\n",
    "# DECODE\n",
    "for _ in range(tokens_b4_prefill[0]):\n",
    "    clm_batch = decode(model, clm_batch, generation_dict, active_ids)\n",
    "\n",
    "iterator = zip(clm_batches[1:], tokens_b4_prefill[1:])\n",
    "for clm_batch_new, tokens_to_generate in iterator:\n",
    "\n",
    "    # PREFILL\n",
    "    clm_batch_new = prefill(model, clm_batch_new, generation_dict)\n",
    "    for r in clm_batch_new.requests:\n",
    "        active_ids.add(r.id)\n",
    "\n",
    "    # CONCATENATE\n",
    "    clm_batch = BLoraCausalLMBatch.concatenate(\n",
    "        batches=[clm_batch, clm_batch_new]\n",
    "    )\n",
    "\n",
    "    # DECODE LOOP\n",
    "    for i in range(tokens_to_generate):\n",
    "        clm_batch = decode(model, clm_batch, generation_dict, active_ids)\n",
    "\n",
    "        if clm_batch is None:\n",
    "            break\n",
    "\n",
    "print_gen(model.tokenizer, input_seqs, generation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>Outline a five sentence short story about the Patriots winning the Super Bowl.\n",
      "1. The New England Patriots, led by their legendary quarterback Tom Brady, were determined to make history once again.\n",
      "2. With their powerful offense and stifling defense, the Pats dominated the AFC Championship against the Kansas City Chiefs.\n",
      "3. In the Super Bowl, they faced off against the Los Angeles Rams, who had an equally impressive roster.\n",
      "4. However, the Patriots' experience\n",
      "\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia\n",
      "\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "<unk><unk><unk><unk><unk>Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer: \\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n < 2) return n;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end\n",
      "\n",
      "\n",
      "\n",
      "<unk>### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = Batch(id=0, requests=requests)\n",
    "clm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model.tokenizer, device=\"cuda\")\n",
    "\n",
    "input_seqs = clm_batch.input_ids.tolist()\n",
    "generation_dict = {req.id : [] for req in clm_batch.requests}\n",
    "\n",
    "active_ids = set()\n",
    "\n",
    "# PREFILL\n",
    "clm_batch = prefill(model, clm_batch, generation_dict)\n",
    "for r in clm_batch.requests:\n",
    "    active_ids.add(r.id)\n",
    "\n",
    "# DECODE\n",
    "while clm_batch is not None:\n",
    "    clm_batch = decode(model, clm_batch, generation_dict, active_ids)\n",
    "\n",
    "print_gen(model.tokenizer, input_seqs, generation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from service.blora_utils import load_loras, prepare_batch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model2 = LlamaForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = 0\n",
    "model2, lora_map = load_loras(model2, lora_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jondurbin/airoboros-7b-gpt4-1.2-peft:\n",
      "Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.\n",
      "The character, who is a young boy named Timmy, stumbles upon a secret room in his house that contained relics from his future. The room was hidden behind a bookcase in the library, and it was filled with strange artifacts and documents.\n",
      "Timmy's curiosity got the best of him, and he decided\n",
      "\n",
      "trl-lib/llama-7b-se-rl-peft:\n",
      "Question: What are the various algorithms to sort a list?\n",
      "\n",
      "Answer:: The most common algorithms are:\n",
      "\\begin{itemize}\n",
      "\\item [Bubble sort](http://en.wikipedia.org/wiki/Bubble_sort)\n",
      "\\item [Selection sort](http://en.wikipedia.org/wiki/Selection_sort)\n",
      "\\item [Insertion sort](http://en.wikipedia\n",
      "\n",
      "winddude/wizardLM-LlaMA-LoRA-7B:\n",
      "### Instruction: Write a poem about the transformers Python library.\n",
      "### Response:\n",
      "Transformers Python library,\n",
      "A powerful tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers Python library,\n",
      "A versatile tool for data processing,\n",
      "It can transform data into new forms,\n",
      "And help us to analyze and visualize.\n",
      "Transformers\n",
      "\n",
      "trl-lib/llama-7b-se-rl-peft:\n",
      "Question: How can I write a Java function to generate the nth Fibonacci number?\n",
      "\n",
      "Answer:0\n",
      "\n",
      "\\begin{code}\n",
      "public static int fib(int n) {\n",
      "    if (n == 0) return 0;\n",
      "    if (n == 1) return 1;\n",
      "    return fib(n - 1) + fib(n - 2);\n",
      "}\n",
      "\\end{code}\n",
      "\n",
      "winddude/wizardLM-LlaMA-LoRA-7B:\n",
      "### Instruction: Develop an eight sentence short story about a character who can bring their dreams into reality. \n",
      "### Response:\n",
      "As the sun rose over the horizon, a young girl named Lily stood on the beach, gazing out at the vast ocean. She had always dreamed of traveling the world, but her family couldn't afford it. That was until she discovered a magical amulet that could bring her dreams to life. With a\n"
     ]
    }
   ],
   "source": [
    "b = prepare_batch(inputs, tokenizer, model2, lora_map)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for out in model2.generate(\n",
    "    **b,\n",
    "    max_length=100,\n",
    "    stream_output=True\n",
    "):\n",
    "    outputs.append(out)\n",
    "    batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))\n",
    "    clear_output(wait=True)\n",
    "    print(\"\\n\\n\".join([lora + \":\\n\" + prompt + decoded for (prompt, lora, _), decoded in zip(inputs, batch_decoded)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unset_batch_lora_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m batch \u001b[39m=\u001b[39m Batch(idx\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, requests\u001b[39m=\u001b[39;49mrequests)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m clm_batch \u001b[39m=\u001b[39m BLoraCausalLMBatch\u001b[39m.\u001b[39mfrom_batch(batch, tokenizer\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mtokenizer, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bquad-pe-1.wisp.internal.neuralmagic.com/home/ubuntu/rshaw/BLoRA-TGI/tgi/example-usage.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m input_seqs \u001b[39m=\u001b[39m clm_batch\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'idx'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = prepare_batch(inputs, tokenizer, model, lora_map)\n",
    "\n",
    "# tokens = b.input_ids[0,:].tolist()\n",
    "\n",
    "# model_kwargs = {\n",
    "#     \"attention_mask\": b.attention_mask,\n",
    "#     \"use_cache\": True\n",
    "# }\n",
    "\n",
    "# input_ids = b.input_ids\n",
    "\n",
    "# for _ in range(100):\n",
    "#     model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "#     outputs = model(\n",
    "#         **model_inputs,\n",
    "#         return_dict=True,\n",
    "#         output_attentions=False,\n",
    "#         output_hidden_states=False,\n",
    "#     )\n",
    "\n",
    "#     next_token_logits = outputs.logits[:, -1, :]\n",
    "#     next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "#     tokens.append(next_tokens.item())\n",
    "\n",
    "#     input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "#     model_kwargs = model._update_model_kwargs_for_generation(\n",
    "#         outputs, model_kwargs, is_encoder_decoder=False\n",
    "#     )\n",
    "\n",
    "#     print(next_tokens.item())\n",
    "#     print(\"\\n\\n\")\n",
    "#     # print(len(tokens))\n",
    "#     # print(tokenizer.decode(tokens[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "Loading LORAs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id=0, requests=[Request(id=0, lora_id='trl-lib/llama-7b-se-rl-peft', inputs='Write a 6 line dialogue between a character and a magical creature that only they can see.')])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "for idx, inp in enumerate(inputs):\n",
    "    requests.append(Request(\n",
    "        id=idx,\n",
    "        lora_id=inp[1],\n",
    "        inputs=inp[0]\n",
    "    ))\n",
    "\n",
    "batch = Batch(\n",
    "    id=0,\n",
    "    requests=requests\n",
    ")\n",
    "\n",
    "causal_lm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model2.tokenizer, device=\"cuda\")\n",
    "generation_dct = {idx: [] for idx in range(len(causal_lm_batch.requests))}\n",
    "\n",
    "for _ in range(100):\n",
    "    # print(causal_lm_batch.input_ids)\n",
    "    # print(causal_lm_batch.position_ids)\n",
    "    # # print(causal_lm_batch.attention_mask)\n",
    "    # if causal_lm_batch.past_key_values is None:\n",
    "    #     print(\"None\")\n",
    "    # else:\n",
    "    #     print(causal_lm_batch.past_key_values[0][0][0,0,0,0])\n",
    "    #     print(causal_lm_batch.past_key_values[0][0][0,0,0,1])\n",
    "\n",
    "    generations, causal_lm_batch = model2.generate_token(causal_lm_batch)\n",
    "    for idx, gen in enumerate(generations):\n",
    "        generation_dct[idx].append(gen.token_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical creature that only they can see.\n",
      "Write a 6 line dialogue between a character and a magical creature that only they can see. Write a 6 line dialogue between a character and a magical\n"
     ]
    }
   ],
   "source": [
    "causal_lm_batch = BLoraCausalLMBatch.from_batch(batch, tokenizer=model2.tokenizer, device=\"cuda\")\n",
    "input_seqs = causal_lm_batch.input_ids.tolist()\n",
    "\n",
    "key = 0\n",
    "input_seqs[key].extend(generation_dct[key])\n",
    "print(model2.tokenizer.decode(input_seqs[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.active_batch_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jondurbin/airoboros-7b-gpt4-1.2-peft']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.model.batch_lora_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in generation_dct:\n",
    "    print(generation_dct[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
